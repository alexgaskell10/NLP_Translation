{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Michael\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Michael\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Michael\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt') # Download the tokenizer model\n",
    "nltk.download('wordnet') # Download the wordnet corpora\n",
    "\n",
    "# CNN, RNN, Forests, SVR, FFNN, linreg\n",
    "\n",
    "data_dir = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train_en = open(data_dir + \"train.ende.src\", \"r\",encoding=\"utf8\")\n",
    "train_en = f_train_en.readlines()\n",
    "\n",
    "f_train_de = open(data_dir + \"train.ende.mt\", \"r\",encoding=\"utf8\")\n",
    "train_de = f_train_de.readlines()\n",
    "\n",
    "f_train_labels = open(data_dir + \"train.ende.scores\", \"r\",encoding=\"utf8\")\n",
    "train_l = f_train_labels.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(strings):\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "    return [tokenizer.tokenize(l) for l in strings]\n",
    "\n",
    "\n",
    "def remove_stopwords(list_strings, additional_stopwords=(), language='english'):\n",
    "    words = stopwords.words('english')\n",
    "    for word in additional_stopwords:\n",
    "        if word not in words: words.append(word)\n",
    "\n",
    "    ret = []\n",
    "    for sentence in list_strings:\n",
    "        ret.append(list(filter(lambda word: word not in words, sentence)))\n",
    "    return ret\n",
    "\n",
    "def remove_capitalisation(list_strings):\n",
    "    ret = []\n",
    "    for sentence in list_strings:\n",
    "        ret.append([w.lower() for w in sentence])\n",
    "    return ret\n",
    "\n",
    "def remove_punctuation(list_strings):\n",
    "    ret = []\n",
    "    for sentence in list_strings:\n",
    "        ret.append(list(filter(lambda word: word not in string.punctuation, sentence)))\n",
    "    return ret\n",
    "\n",
    "\n",
    "def build_vocab(list_string):\n",
    "    vocab_set = set()\n",
    "    for sentence in list_string:\n",
    "        vocab_set.update(sentence)\n",
    "        \n",
    "    return vocab_set\n",
    "\n",
    "def build_w2i_i2w(vocabulary):\n",
    "    word2idx = {w: idx+1 for (idx, w) in enumerate(vocabulary)}\n",
    "    word2idx['<pad>'] = 0\n",
    "    idx2word = {idx+1: w for (idx, w) in enumerate(vocabulary)}\n",
    "    word2idx[0] = '<pad>'\n",
    "    return word2idx, idx2word\n",
    "\n",
    "def lemmatise():\n",
    "    pass\n",
    "\n",
    "def stem():\n",
    "    pass\n",
    "\n",
    "def extract_context(list_string, word2idx, window_size=2):\n",
    "    idx_pairs = []\n",
    "    for sentence in list_string:\n",
    "        indices = [word2idx[word] for word in sentence]\n",
    "\n",
    "        for center_word_pos in range(len(indices)):\n",
    "\n",
    "            for w in range(-window_size, window_size + 1):\n",
    "                context_word_pos = center_word_pos + w\n",
    "\n",
    "                if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                    continue\n",
    "\n",
    "                context_word_idx = indices[context_word_pos]\n",
    "                idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "    return np.array(idx_pairs) # it will be useful to have this as numpy array\n",
    "\n",
    "\n",
    "def get_one_hot(vocabulary, word_idx):\n",
    "    x = torch.zeros(len(vocabulary)).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x\n",
    "\n",
    "def preprocessing(data_list):\n",
    "    train_tokenized = tokenize(data_list)\n",
    "    train_nostop = remove_stopwords(train_tokenized)\n",
    "    train_nopunct = remove_punctuation(train_nostop)\n",
    "    train_nocap = remove_capitalisation(train_nopunct)\n",
    "    return train_nocap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_tokenised_en = preprocessing(train_en)\n",
    "train_tokenised_de = preprocessing(train_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_en = build_vocab(train_tokenised_en)\n",
    "word2idx_en, idx2word_en = build_w2i_i2w(voc_en)\n",
    "\n",
    "voc_de = build_vocab(train_tokenised_de)\n",
    "word2idx_de, idx2word_de = build_w2i_i2w(voc_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contexts = extract_context(train_nocap, word2idx, window_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_inputs(tokenized_corpus, word2idx):\n",
    "    # we index our sentences\n",
    "    vectorized_sents = [[word2idx[tok] for tok in sent if tok in word2idx] for sent in tokenized_corpus]\n",
    "\n",
    "    # Sentence lengths\n",
    "    sent_lengths = [len(sent) for sent in vectorized_sents]\n",
    "\n",
    "    # Get maximum length\n",
    "    max_len = max(sent_lengths)\n",
    "\n",
    "    # we create a tensor of a fixed size filled with zeroes for padding\n",
    "    sent_tensor = torch.zeros((len(vectorized_sents), max_len)).long()\n",
    "\n",
    "    # we fill it with our vectorized sentences \n",
    "    for idx, (sent, sentlen) in enumerate(zip(vectorized_sents, sent_lengths)):\n",
    "        sent_tensor[idx, :sentlen] = torch.LongTensor(sent)\n",
    "\n",
    "    return sent_tensor\n",
    "\n",
    "train_sent_tensor_en = get_model_inputs(train_tokenised_en, word2idx_en)\n",
    "train_sent_tensor_de = get_model_inputs(train_tokenised_de, word2idx_de)\n",
    "labels_tensor = torch.FloatTensor([float(t.strip()) for t in train_l])\n",
    "\n",
    "train_tensor = torch.cat((train_sent_tensor_en, train_sent_tensor_de), 1)\n",
    "\n",
    "\n",
    "val_l = open(data_dir + \"dev.ende.scores\", \"r\",encoding=\"utf8\")\n",
    "val_l = val_l.readlines()\n",
    "val_l = torch.FloatTensor([float(t.strip()) for t in val_l])\n",
    "\n",
    "val_en = open(data_dir + \"dev.ende.src\", \"r\",encoding=\"utf8\")\n",
    "val_en = val_en.readlines()\n",
    "val_en = preprocessing(val_en)\n",
    "\n",
    "val_de = open(data_dir + \"dev.ende.mt\", \"r\",encoding=\"utf8\")\n",
    "val_de = val_de.readlines()\n",
    "val_de = preprocessing(val_de)\n",
    "\n",
    "val_tensor_en = get_model_inputs(val_en, word2idx_en)\n",
    "val_tensor_de = get_model_inputs(val_de, word2idx_de)\n",
    "\n",
    "val_tensor = torch.cat((val_tensor_en, val_tensor_de), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, num_classes):  \n",
    "        super(FFNN, self).__init__()\n",
    "        \n",
    "        # embedding (lookup layer) layer\n",
    "        # padding_idx argument makes sure that the 0-th token in the vocabulary\n",
    "        # is used for padding purposes i.e. its embedding will be a 0-vector\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # hidden layer\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # output layer\n",
    "        self.fc3 = nn.Linear(hidden_dim, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x has shape (batch_size, max_sent_len)\n",
    "\n",
    "        embedded = self.embedding(x)\n",
    "        # `embedding` has shape (batch size, max_sent_len, embedding dim)\n",
    "\n",
    "        ########################################################################\n",
    "        # Q: Compute the average embeddings of shape (batch_size, embedding_dim)\n",
    "        ########################################################################\n",
    "        # Implement averaging that ignores padding (average using actual sentence lengths).\n",
    "        # How this effect the result?\n",
    "#         avg = torch.zeros((embedded.size(0), embedded.size(2)))\n",
    "#         for i, row in enumerate(embedded):\n",
    "#             avg[i] = torch.sum(row,0) / (row[row != 0].size(0) / 100)\n",
    "#             if i%1000 == 0 and i != 0: print(i)\n",
    "        averaged = torch.sum(embedded, 1) / torch.sum(embedded != 0,1)\n",
    "\n",
    "        out = F.leaky_relu(self.fc1(averaged))\n",
    "        out = F.leaky_relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        return torch.sqrt(self.mse(yhat,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will train for 200 epochs\n",
      "| Epoch: 10 | Train Loss: 0.822 | Validation Loss: 0.862\n",
      "| Epoch: 20 | Train Loss: 0.815 | Validation Loss: 0.862\n",
      "| Epoch: 30 | Train Loss: 0.797 | Validation Loss: 0.869\n",
      "| Epoch: 40 | Train Loss: 0.765 | Validation Loss: 0.894\n",
      "| Epoch: 50 | Train Loss: 0.711 | Validation Loss: 0.921\n",
      "| Epoch: 60 | Train Loss: 0.628 | Validation Loss: 0.968\n",
      "| Epoch: 70 | Train Loss: 0.515 | Validation Loss: 1.027\n",
      "| Epoch: 80 | Train Loss: 0.389 | Validation Loss: 1.085\n",
      "| Epoch: 90 | Train Loss: 0.276 | Validation Loss: 1.124\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-c404b09fa42a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m# calculate the gradient of each parameter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;31m# update the parameters using the gradients and optimizer algorithm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Program_Files\\Anaconda\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Program_Files\\Anaconda\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "LRATE = 0.001\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 100\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "model = FFNN(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx_en) + len(word2idx_de), OUTPUT_DIM)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LRATE)\n",
    "\n",
    "\n",
    "loss_fn = RMSELoss()\n",
    "\n",
    "# Input and label tensors\n",
    "feature = train_tensor\n",
    "target = labels_tensor\n",
    "validation = val_tensor\n",
    "val_labels = val_l\n",
    "\n",
    "################\n",
    "# Start training\n",
    "################\n",
    "print(f'Will train for {EPOCHS} epochs')\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # to ensure the dropout (explained later) is \"turned on\" while training\n",
    "    # good practice to include even if do not use here\n",
    "    model.train()\n",
    "  \n",
    "    # we zero the gradients as they are not removed automatically\n",
    "    optimizer.zero_grad()\n",
    "  \n",
    "    # squeeze is needed as the predictions will have the shape (batch size, 1)\n",
    "    # and we need to remove the dimension of size 1\n",
    "    predictions = model(feature).squeeze(1)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = loss_fn(predictions, target)\n",
    "    train_loss = loss.item()\n",
    "\n",
    "    # calculate the gradient of each parameter\n",
    "    loss.backward()\n",
    "\n",
    "    # update the parameters using the gradients and optimizer algorithm \n",
    "    optimizer.step()\n",
    "  \n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        with torch.no_grad():\n",
    "            v_pred = model(validation).squeeze(1)\n",
    "            v_loss = loss_fn(v_pred, val_labels)\n",
    "            val_loss = v_loss.item()\n",
    "            print(f'| Epoch: {epoch:02d} | Train Loss: {train_loss:.3f} | Validation Loss: {val_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
