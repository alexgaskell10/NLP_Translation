{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/alexgaskell10/NLP_Translation/blob/master/notebooks/baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "omEQHVdOS61G"
   },
   "source": [
    "# Coursework: Baseline Model\n",
    "\n",
    "This notebook takes you step by step to the implementation of a simple baseline model to get you started on the coursework. You have a section for the English-German task and another for English-Chinese. They are made to be standalone so feel free to check only one of the sections. However, as the tasks require slighlty different approaches, going through both sections could help you to get inspired for your chosen task, especially each task processes english in a slighlty different way.\n",
    "\n",
    "Enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lweXud1Wpemd"
   },
   "source": [
    "## A. English-German"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yu6s3YOf_C93"
   },
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "colab_type": "code",
    "id": "scs7ICZrPFcs",
    "outputId": "257cf544-ed16-448a-d70c-934ccf112bdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-07 08:35:26--  https://competitions.codalab.org/my/datasets/download/c748d2c0-d6be-4e36-9f12-ca0e88819c4d\n",
      "Resolving competitions.codalab.org (competitions.codalab.org)... 129.175.22.230\n",
      "Connecting to competitions.codalab.org (competitions.codalab.org)|129.175.22.230|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 FOUND\n",
      "Location: https://newcodalab.lri.fr/prod-private/dataset_data_file/None/104ea/en-de.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=edd0aa1a83e65dd1db791aed965a0edde71751b15380268ce049767cc36065dc&X-Amz-Date=20200207T083527Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200207%2Fnewcodalab%2Fs3%2Faws4_request [following]\n",
      "--2020-02-07 08:35:27--  https://newcodalab.lri.fr/prod-private/dataset_data_file/None/104ea/en-de.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=edd0aa1a83e65dd1db791aed965a0edde71751b15380268ce049767cc36065dc&X-Amz-Date=20200207T083527Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200207%2Fnewcodalab%2Fs3%2Faws4_request\n",
      "Resolving newcodalab.lri.fr (newcodalab.lri.fr)... 129.175.15.11\n",
      "Connecting to newcodalab.lri.fr (newcodalab.lri.fr)|129.175.15.11|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 864010 (844K) [application/zip]\n",
      "Saving to: ‘ende_data.zip’\n",
      "\n",
      "ende_data.zip       100%[===================>] 843.76K  1.51MB/s    in 0.5s    \n",
      "\n",
      "2020-02-07 08:35:28 (1.51 MB/s) - ‘ende_data.zip’ saved [864010/864010]\n",
      "\n",
      "Archive:  ende_data.zip\n",
      "  inflating: dev.ende.mt             \n",
      "  inflating: dev.ende.scores         \n",
      "  inflating: dev.ende.src            \n",
      "  inflating: test.ende.mt            \n",
      "  inflating: test.ende.src           \n",
      "  inflating: train.ende.mt           \n",
      "  inflating: train.ende.scores       \n",
      "  inflating: train.ende.src          \n"
     ]
    }
   ],
   "source": [
    "# Download and unzip the data\n",
    "from os.path import exists\n",
    "if not exists('ende_data.zip'):\n",
    "    !wget -O ende_data.zip https://competitions.codalab.org/my/datasets/download/c748d2c0-d6be-4e36-9f12-ca0e88819c4d\n",
    "    !unzip ende_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "colab_type": "code",
    "id": "jPy_iwHnOSAZ",
    "outputId": "b57668f1-f8f7-4210-f0dc-ec2f04a46611"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---EN-DE---\n",
      "\n",
      "Source:  José Ortega y Gasset visited Husserl at Freiburg in 1934.\n",
      "\n",
      "Translation:  1934 besuchte José Ortega y Gasset Husserl in Freiburg.\n",
      "\n",
      "Score:  1.1016968715664406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the files\n",
    "import io\n",
    "\n",
    "#English-German\n",
    "print(\"---EN-DE---\")\n",
    "print()\n",
    "\n",
    "with open(\"./train.ende.src\", \"r\") as ende_src:\n",
    "    print(\"Source: \",ende_src.readline())\n",
    "with open(\"./train.ende.mt\", \"r\") as ende_mt:\n",
    "    print(\"Translation: \",ende_mt.readline())\n",
    "with open(\"./train.ende.scores\", \"r\") as ende_scores:\n",
    "    print(\"Score: \",ende_scores.readline())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wiFHVnfH_Jpv"
   },
   "source": [
    "### Computing Sentence Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g05fv5GiSyQ4"
   },
   "source": [
    "For this baseline model, we will simply use pre-trained GloVe embeddings via the Spacy module and compute the vector for each word and take the global mean for each sentence. We will do the same for both source and translation sentences. For chinese tokenization and embeddings we will have to find other tools.\n",
    "\n",
    "This is a very simplistic approach so feel free to be more creative and play around with how the sentence embeddings are computed for example ;)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jcjZpNlra8TD"
   },
   "source": [
    "GloVe embeddings do not support the Chinese language so in the section of the English-Chinese task we will have to download pretrained Chinese embeddings from word2vec repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "colab_type": "code",
    "id": "96bRtBbuZLJe",
    "outputId": "97af134a-99fa-4834-9251-b48272b69d17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_md==2.1.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz (95.4MB)\n",
      "\u001b[K     |████████████████████████████████| 95.4MB 73.4MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: en-core-web-md\n",
      "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for en-core-web-md: filename=en_core_web_md-2.1.0-cp36-none-any.whl size=97126236 sha256=dc9ae2c79d0806387537e7d3c9401e31a25f8db3e43711bd6561ff570a06e9ad\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-j34p9wv6/wheels/c1/2c/5f/fd7f3ec336bf97b0809c86264d2831c5dfb00fc2e239d1bb01\n",
      "Successfully built en-core-web-md\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-2.1.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_md')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.6/dist-packages/en_core_web_md -->\n",
      "/usr/local/lib/python3.6/dist-packages/spacy/data/en300\n",
      "You can now load the model via spacy.load('en300')\n",
      "Collecting de_core_news_md==2.1.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_md-2.1.0/de_core_news_md-2.1.0.tar.gz (220.8MB)\n",
      "\u001b[K     |████████████████████████████████| 220.8MB 154.8MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: de-core-news-md\n",
      "  Building wheel for de-core-news-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for de-core-news-md: filename=de_core_news_md-2.1.0-cp36-none-any.whl size=224546880 sha256=499bf1246e222e74abea49f03c3bc06a7017957d3d1618fe70cba1f3ab061b03\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-im4yi74g/wheels/44/34/f1/31d4b0fa32008c09695ccb180865f196ecd9d512c146f99749\n",
      "Successfully built de-core-news-md\n",
      "Installing collected packages: de-core-news-md\n",
      "Successfully installed de-core-news-md-2.1.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('de_core_news_md')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.6/dist-packages/de_core_news_md -->\n",
      "/usr/local/lib/python3.6/dist-packages/spacy/data/de300\n",
      "You can now load the model via spacy.load('de300')\n"
     ]
    }
   ],
   "source": [
    "#Downloading spacy models for english and german\n",
    "\n",
    "!spacy download en_core_web_md\n",
    "!spacy link en_core_web_md en300\n",
    "\n",
    "!spacy download de_core_news_md\n",
    "!spacy link de_core_news_md de300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Om6kQX5bX2mB"
   },
   "source": [
    "We can now write our functions that will return the average embeddings for a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nhT2I6WYavY4"
   },
   "source": [
    "#### Pre-processing with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "id": "19gsNCgnW8ZT",
    "outputId": "87661d04-5707-4cbe-d973-992eea55f643"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#downloading stopwords from the nltk package\n",
    "download('stopwords') #stopwords dictionary, run once\n",
    "\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "stop_words_de = set(stopwords.words('german'))\n",
    "\n",
    "def get_sentence_emb(line,nlp,lang):\n",
    "    if lang == 'en':\n",
    "        text = line.lower()\n",
    "        l = [token.lemma_ for token in nlp.tokenizer(text)]\n",
    "        l = ' '.join([word for word in l if word not in stop_words_en])\n",
    "\n",
    "    elif lang == 'de':\n",
    "        text = line.lower()\n",
    "        l = [token.lemma_ for token in nlp.tokenizer(text)]\n",
    "        l = ' '.join([word for word in l if word not in stop_words_de])\n",
    "\n",
    "    sen = nlp(l)\n",
    "    return sen.vector\n",
    "\n",
    "def get_embeddings(f,nlp,lang):\n",
    "    file = open(f) \n",
    "    lines = file.readlines() \n",
    "    sentences_vectors =[]\n",
    "\n",
    "    for l in lines:\n",
    "        vec = get_sentence_emb(l,nlp,lang)\n",
    "        if vec is not None:\n",
    "            vec = np.mean(vec)\n",
    "            sentences_vectors.append(vec)\n",
    "        else:\n",
    "            print(\"didn't work :\", l)\n",
    "            sentences_vectors.append(0)\n",
    "\n",
    "    return sentences_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NUKMgbo2sreI"
   },
   "source": [
    "#### Getting Training and Validation Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZXqZamIKs30T"
   },
   "source": [
    "We will now run the code fo the English-German translations and getting our training and validation sets ready for the regression task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GyJr7cIkQ3E8"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_de = spacy.load('de300')\n",
    "nlp_en = spacy.load('en300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "LwoUIDj0otbf",
    "outputId": "e2537b97-867e-4b53-caca-1a708c2fdcfd"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-b032b22a6d0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#EN-DE files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mde_train_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./train.ende.src\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnlp_en\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mde_train_mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./train.ende.mt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnlp_de\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'de'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-c759160c42bc>\u001b[0m in \u001b[0;36mget_embeddings\u001b[0;34m(f, embeddings, lang)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m       \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sentence_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-c759160c42bc>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(sentence, nlp)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words_en\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#restricts string to alphabetic characters only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'tokenizer'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#EN-DE files\n",
    "de_train_src = get_embeddings(\"./train.ende.src\",nlp_en,'en')\n",
    "de_train_mt = get_embeddings(\"./train.ende.mt\",nlp_de,'de')\n",
    "\n",
    "f_train_scores = open(\"./train.ende.scores\",'r')\n",
    "de_train_scores = f_train_scores.readlines()\n",
    "\n",
    "de_val_src = get_embeddings(\"./dev.ende.src\",nlp_en,'en')\n",
    "de_val_mt = get_embeddings(\"./dev.ende.mt\",nlp_de,'de')\n",
    "f_val_scores = open(\"./dev.ende.scores\",'r')\n",
    "de_val_scores = f_val_scores.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U_K1CHl5VxiE"
   },
   "outputs": [],
   "source": [
    "#EN-DE\n",
    "print(f\"Training mt: {len(de_train_mt)} Training src: {len(de_train_src)}\")\n",
    "print()\n",
    "print(f\"Validation mt: {len(de_val_mt)} Validation src: {len(de_val_src)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Px7ikaGoy9r0"
   },
   "outputs": [],
   "source": [
    "#Put the features into a list\n",
    "X_train = [np.array(de_train_src),np.array(de_train_mt)]\n",
    "X_train_de = np.array(X_train).transpose()\n",
    "\n",
    "X_val = [np.array(de_val_src),np.array(de_val_mt)]\n",
    "X_val_de = np.array(X_val).transpose()\n",
    "\n",
    "#Scores\n",
    "train_scores = np.array(de_train_scores).astype(float)\n",
    "y_train_de =train_scores\n",
    "\n",
    "val_scores = np.array(de_val_scores).astype(float)\n",
    "y_val_de = val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "RFp6yyBl4Kgf",
    "outputId": "3cdd6417-5423-4dfb-e268-27646bd226c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nX_train= [np.array(train_src),np.array(train_mt)]\\nX_train = np.array(X_train)\\n\\n\\nX_test = [np.array(test_src),np.array(test_mt)]\\nX_test = np.array(X_test)\\n\\n\\n#Reshaping if using shape >3\\nnsamples, nx, ny = X_train.shape\\nX_train = X_train.reshape((nx,ny*nsamples))\\n\\nnsamples, nx, ny = X_test.shape\\nX_test = X_test.reshape((nx,ny*nsamples))\\n\\nprint(X_train.shape)\\nprint(X_test.shape)\\n\\n\\n\\n#Scores\\ntrain_scores = np.array(train_scores).astype(float)\\ny_train =train_scores\\n\\ntest_scores = np.array(test_scores).astype(float)\\ny_test =test_scores\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN IF WANT TO HAVE AVERAGE VECTOR(AND NOT GLOBAL MEAN), THIS GAVE WORSE PERFORMANCE THAN GLOBAL MEAN\n",
    "'''\n",
    "\n",
    "X_train= [np.array(train_src),np.array(train_mt)]\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "\n",
    "X_test = [np.array(test_src),np.array(test_mt)]\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "\n",
    "#Reshaping if using shape >3\n",
    "nsamples, nx, ny = X_train.shape\n",
    "X_train = X_train.reshape((nx,ny*nsamples))\n",
    "\n",
    "nsamples, nx, ny = X_test.shape\n",
    "X_test = X_test.reshape((nx,ny*nsamples))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "#Scores\n",
    "train_scores = np.array(train_scores).astype(float)\n",
    "y_train =train_scores\n",
    "\n",
    "test_scores = np.array(test_scores).astype(float)\n",
    "y_test =test_scores\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XSIE7d8HCTpi"
   },
   "source": [
    "### Training the Regressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3eoY14lNCTe3"
   },
   "source": [
    "At this point,  will try SVM and Random Tree Forests and choose the model with the highest Pearson correlation.\n",
    "\n",
    "First we will define our RMSE function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dRcegRvW2F2q"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IerDa2251swL"
   },
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "exHbrWtq14jm"
   },
   "source": [
    "SVM have many parameters such as the kernel and the regularizating constant C. Here we will use C = 1 and compare kernels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "QiHCkGUgsJ8r",
    "outputId": "97bedc90-75ff-411c-929c-dc095eedab45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear\n",
      "RMSE: 0.8813940062646772 Pearson 0.05474933605816939\n",
      "\n",
      "poly\n",
      "RMSE: 0.881237789938125 Pearson 0.06200263088261973\n",
      "\n",
      "rbf\n",
      "RMSE: 0.881693995276586 Pearson 0.023417950678245963\n",
      "\n",
      "sigmoid\n",
      "RMSE: 349.7715466025825 Pearson -0.037125683901182906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "for k in ['linear','poly','rbf','sigmoid']:\n",
    "    clf_t = SVR(kernel=k)\n",
    "    clf_t.fit(X_train_de, y_train_de)\n",
    "    print(k)\n",
    "    predictions = clf_t.predict(X_val_de)\n",
    "    pearson = pearsonr(y_val_de, predictions)\n",
    "    print(f'RMSE: {rmse(predictions,y_val_de)} Pearson {pearson[0]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P1Fr1RLm3-Kc"
   },
   "source": [
    "Here the best kernel seems to be the polynomial one as it gives us the highest pearson correlation at 0.062."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qg9YSBUG1zaL"
   },
   "source": [
    "#### Random Tree Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Au88fVS33K4W"
   },
   "source": [
    "Another powerful regressor is the Random Tree Forest. Here we have to choose the number of trees we want to compute and we will pick n_estimators = 1000. The higher the number the longer it will compute. To fine tune that number you could compute the error per number of trees and select the number for which there is no more significant improvement( the \"elbow\" of the graph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "id": "VOld4zbmsOGL",
    "outputId": "fe1089ff-07bb-4d7c-c638-a8100753ab05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9401130737723049\n",
      "Pearson -0.03680114854324954\n"
     ]
    }
   ],
   "source": [
    "# Import the model we are using\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 666)\n",
    "\n",
    "rf.fit(X_train_de, y_train_de);\n",
    "\n",
    "predictions = rf.predict(X_val_de)\n",
    "\n",
    "pearson = pearsonr(y_val_de, predictions)\n",
    "print('RMSE:', rmse(predictions,y_val_de))\n",
    "print(f\"Pearson {pearson[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0L73jhAc6ZoM"
   },
   "source": [
    "In this case, it seems like the SVM with a linear kernel performed the best on our validation set so we will save that model for the test set predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G9puD_0zkC2c"
   },
   "source": [
    "### Writing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oQvvIhPDkUnR"
   },
   "source": [
    "Here is our function to write the scores into a txt file. We can follow the <Method> <ID> <SCORE> template but having only the scores will work too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LN3NtkF4kPxw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def writeScores(method_name, scores):\n",
    "    fn = \"predictions.txt\"\n",
    "    print(\"\")\n",
    "    with open(fn, 'w') as output_file:\n",
    "        for idx,x in enumerate(scores):\n",
    "            #out =  metrics[idx]+\":\"+str(\"{0:.2f}\".format(x))+\"\\n\"\n",
    "            #print(out)\n",
    "            output_file.write(f\"{x}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FVss_RLBkFei"
   },
   "outputs": [],
   "source": [
    "#EN-DE\n",
    "\n",
    "de_test_src = get_embeddings(\"./test.ende.src\",nlp_en,'en')\n",
    "de_test_mt = get_embeddings(\"./test.ende.mt\",nlp_de,'de')\n",
    "\n",
    "X= [np.array(de_test_src),np.array(de_test_mt)]\n",
    "X_test = np.array(X).transpose()\n",
    "\n",
    "#Predict\n",
    "clf_de = SVR(kernel='rbf')\n",
    "clf_de.fit(X_train_de, y_train_de)\n",
    "\n",
    "predictions_de = clf_de.predict(X_val_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "id": "XWnNUR0Gku_9",
    "outputId": "5904152f-30ee-43d3-ea59-0da373c9c021"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "from zipfile import ZipFile\n",
    "\n",
    "writeScores(\"SVR\",predictions_de)\n",
    "\n",
    "with ZipFile(\"en-de_svr.zip\",\"w\") as newzip:\n",
    "    newzip.write(\"predictions.txt\")\n",
    " \n",
    "files.download('en-de_svr.zip') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wyaM_P0bynB-"
   },
   "source": [
    "### Results\n",
    "\n",
    "Once submitted to codalab, the pearson correlation is 0.0052."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3obhUYW5ptUS"
   },
   "source": [
    "##B. English-Chinese\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OE9wypehaLrZ"
   },
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "colab_type": "code",
    "id": "_5y34iNipyr3",
    "outputId": "1f404e62-b589-4db5-ebc7-30a07937166c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-07 08:42:15--  https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
      "Resolving competitions.codalab.org (competitions.codalab.org)... 129.175.22.230\n",
      "Connecting to competitions.codalab.org (competitions.codalab.org)|129.175.22.230|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 FOUND\n",
      "Location: https://newcodalab.lri.fr/prod-private/dataset_data_file/None/630ec/en-zh.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=3add5877798fb6ad81b05bbb1a80936898d690b048df22de322955c53f92ad00&X-Amz-Date=20200207T084215Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200207%2Fnewcodalab%2Fs3%2Faws4_request [following]\n",
      "--2020-02-07 08:42:15--  https://newcodalab.lri.fr/prod-private/dataset_data_file/None/630ec/en-zh.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=3add5877798fb6ad81b05bbb1a80936898d690b048df22de322955c53f92ad00&X-Amz-Date=20200207T084215Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200207%2Fnewcodalab%2Fs3%2Faws4_request\n",
      "Resolving newcodalab.lri.fr (newcodalab.lri.fr)... 129.175.15.11\n",
      "Connecting to newcodalab.lri.fr (newcodalab.lri.fr)|129.175.15.11|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 870893 (850K) [application/zip]\n",
      "Saving to: ‘enzh_data.zip’\n",
      "\n",
      "enzh_data.zip       100%[===================>] 850.48K  1.51MB/s    in 0.5s    \n",
      "\n",
      "2020-02-07 08:42:16 (1.51 MB/s) - ‘enzh_data.zip’ saved [870893/870893]\n",
      "\n",
      "Archive:  enzh_data.zip\n",
      "  inflating: dev.enzh.mt             \n",
      "  inflating: dev.enzh.scores         \n",
      "  inflating: dev.enzh.src            \n",
      "  inflating: test.enzh.mt            \n",
      "  inflating: test.enzh.src           \n",
      "  inflating: train.enzh.mt           \n",
      "  inflating: train.enzh.src          \n",
      "  inflating: train.enzh.scores       \n"
     ]
    }
   ],
   "source": [
    "from os.path import exists\n",
    "\n",
    "if not exists('enzh_data.zip'):\n",
    "    !wget -O enzh_data.zip https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
    "    !unzip enzh_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "colab_type": "code",
    "id": "RlXMiqJXq8fy",
    "outputId": "18757b14-06ae-42eb-878a-aee8da0748a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---EN-ZH---\n",
      "\n",
      "Source:  The last conquistador then rides on with his sword drawn.\n",
      "\n",
      "Translation:  最后的征服者骑着他的剑继续前进.\n",
      "\n",
      "Score:  -1.5284005772625449\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#English-Chinese\n",
    "#Checking Data\n",
    "print(\"---EN-ZH---\")\n",
    "print()\n",
    "\n",
    "with open(\"./train.enzh.src\", \"r\") as enzh_src:\n",
    "    print(\"Source: \",enzh_src.readline())\n",
    "with open(\"./train.enzh.mt\", \"r\") as enzh_mt:\n",
    "    print(\"Translation: \",enzh_mt.readline())\n",
    "with open(\"./train.enzh.scores\", \"r\") as enzh_scores:\n",
    "    print(\"Score: \",enzh_scores.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GlOd_5a6aTVP"
   },
   "source": [
    "### Computing Sentence Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZgqjMa_wu0xz"
   },
   "source": [
    "For this task, we will compute the embeddings for words in a sentence in one language and compute the global mean for that sentence, and do the same for the other language. However, we will have to find and download pre-traind embeddings for Chinese as Spacy nor GloVe handle it. The embeddings we will be using for Chinese are of dimension 100, therefore we need to adapt the embeddings for english from the dim 300 to 100. Glove does have English embeddings of dim 100 but Spacy does not have that model. So, we will tokenize the sentences using Spacy tokenizer and use GloVe directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QsKYMxCSolrx"
   },
   "source": [
    "#### Pre-processing English with GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xd24p41jkv7N"
   },
   "source": [
    "With GloVe's function *stoi()* (string to int) we can get the index corresponding to a given word and with the function *itos()* we get the word given its index. To obtain the vector of a word we first get the integer associated with it and then index it into the word embedding tensor with that index. Note that glove takes words in a lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "8lc4rdJnrE_Q",
    "outputId": "c57f48cc-3386-45ea-be81-01f2238e863b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_md==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz#egg=en_core_web_md==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_md')\n",
      "\n",
      "\u001b[38;5;1m✘ Link 'en300' already exists\u001b[0m\n",
      "To overwrite an existing link, use the --force flag\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DON'T RUN IF YOU ALREADY RAN IT IN THE ENGLISH-GERMAN SECTION\n",
    "# Downloading spacy models for english\n",
    "\n",
    "!spacy download en_core_web_md\n",
    "!spacy link en_core_web_md en300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "id": "fx3Ja9zWFDj2",
    "outputId": "e9a01d7f-c751-41dc-d8b8-9ac30b9c214b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [06:28, 2.22MB/s]                          \n",
      "100%|█████████▉| 399054/400000 [00:33<00:00, 22369.37it/s]"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "import spacy\n",
    "\n",
    "#Embeddings\n",
    "glove = torchtext.vocab.GloVe(name='6B', dim=100)\n",
    "\n",
    "#tokenizer model\n",
    "nlp_en =spacy.load('en300')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lxjmj7vUv08E"
   },
   "source": [
    "We can now write our functions that will return the average embeddings for a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "id": "2BUi2QiCIi9y",
    "outputId": "638b3944-99fa-4fb7-e99c-abfe7eeb668a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#ENGLISH EMBEDDINGS methods from the section GERMAN-ENGLISH\n",
    "# The difference from previous section is that we will use Glove embeddings directly because we are using a smaller model that spacy doesn't have\n",
    "# We add a method to compute the word embedding and a method to compute the sentence embedding by averaging the word vectors\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#downloading stopwords from the nltk package\n",
    "download('stopwords') #stopwords dictionary, run once\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def preprocess(sentence,nlp):\n",
    "    text = sentence.lower()\n",
    "    doc = [token.lemma_ for token in  nlp.tokenizer(text)]\n",
    "    doc = [word for word in doc if word not in stop_words_en]\n",
    "    doc = [word for word in doc if word.isalpha()] #restricts string to alphabetic characters only\n",
    "    return doc\n",
    "\n",
    "def get_word_vector(embeddings, word):\n",
    "    try:\n",
    "        vec = embeddings.vectors[embeddings.stoi[word]]\n",
    "        return vec\n",
    "    except KeyError:\n",
    "        #print(f\"Word {word} does not exist\")\n",
    "        pass\n",
    "\n",
    "def get_sentence_vector(embeddings,line):\n",
    "    vectors = []\n",
    "    for w in line:\n",
    "        emb = get_word_vector(embeddings,w)\n",
    "        #do not add if the word is out of vocabulary\n",
    "        if emb is not None:\n",
    "            vectors.append(emb)\n",
    "   \n",
    "    return torch.mean(torch.stack(vectors))\n",
    "\n",
    "\n",
    "def get_embeddings(f,embeddings,lang):\n",
    "    file = open(f) \n",
    "    lines = file.readlines() \n",
    "    sentences_vectors =[]\n",
    "\n",
    "    for l in lines:\n",
    "        sentence= preprocess(l,lang)\n",
    "        try:\n",
    "            vec = get_sentence_vector(embeddings,sentence)\n",
    "            sentences_vectors.append(vec)\n",
    "        except:\n",
    "            sentences_vectors.append(0)\n",
    "\n",
    "    return sentences_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f4JnbxSaaasu"
   },
   "source": [
    "#### Loading Chinese Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t3-NpUxd52nP"
   },
   "source": [
    "We now have to download the pre-trained embeddings for Chinese. We will get them from the University of Oslo NLPL repository (http://vectors.nlpl.eu/repository/), which has word2vec vectors of dimension 100.\n",
    "\n",
    " We will also get Chinese stop words from https://github.com/Tony607/Chinese_sentiment_analysis.\n",
    "\n",
    "For embeddings of dimensions 300 you can find them searching on github repositories. One example is https://github.com/Kyubyong/wordvectors.\n",
    "\n",
    "If you want to work on colab and download other embeddings I would suggest you download the file and upload it on your dropbox and get the link from there.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "colab_type": "code",
    "id": "-jW3S2-rs6BV",
    "outputId": "39aa36d5-0a1f-4cc5-84d6-433f89a87ebf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-07 08:49:53--  https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt\n",
      "Resolving github.com (github.com)... 192.30.253.113\n",
      "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘chinese_stop_words.txt’\n",
      "\n",
      "chinese_stop_words.     [ <=>                ] 419.42K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2020-02-07 08:49:54 (6.55 MB/s) - ‘chinese_stop_words.txt’ saved [429483]\n",
      "\n",
      "--2020-02-07 08:49:55--  http://vectors.nlpl.eu/repository/20/35.zip\n",
      "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.225\n",
      "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.225|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1458485917 (1.4G) [application/zip]\n",
      "Saving to: ‘zh.zip’\n",
      "\n",
      "zh.zip              100%[===================>]   1.36G  23.1MB/s    in 64s     \n",
      "\n",
      "2020-02-07 08:51:00 (21.6 MB/s) - ‘zh.zip’ saved [1458485917/1458485917]\n",
      "\n",
      "Archive:  zh.zip\n",
      "  inflating: LIST                    \n",
      "  inflating: meta.json               \n",
      "  inflating: model.bin               \n",
      "  inflating: model.txt               \n",
      "  inflating: README                  \n"
     ]
    }
   ],
   "source": [
    "!wget -c https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt\n",
    "\n",
    "!wget -O zh.zip http://vectors.nlpl.eu/repository/20/35.zip\n",
    "\n",
    "!unzip zh.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AQM6Go4rEe9N"
   },
   "source": [
    "We now load the pre-trained word2vec embeddings we downloaded using the gensim package. More info on gensim and how to use it to load models and embeddings here https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "uDUbXQ4aMv1K",
    "outputId": "e4e092e6-678c-4532-c481-6d3eb114359e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "wv_from_bin = KeyedVectors.load_word2vec_format(\"model.bin\", binary=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uhZ3HtrdodcW"
   },
   "source": [
    "#### Pre-processing Chinese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "her8c6oJFWAa"
   },
   "source": [
    "For pre-processing chinese sentence we will use the tokenizer package for chinese called jieba and use the downloaded list of chinese stop words to remove them from our tokens. More info on jieba and its options at https://github.com/fxsjy/jieba. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-LA9N1zgsSQl"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import jieba\n",
    "import gensim \n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "stop_words = [ line.rstrip() for line in open('./chinese_stop_words.txt',\"r\", encoding=\"utf-8\") ]\n",
    "\n",
    "\n",
    "def get_sentence_vector_zh(line):\n",
    "    vectors = []\n",
    "    for w in line:\n",
    "        try:\n",
    "            emb = wv_from_bin[w]\n",
    "            vectors.append(emb)\n",
    "        except:\n",
    "            pass #Do not add if the word is out of vocabulary\n",
    "    if vectors:\n",
    "        vectors = np.array(vectors)\n",
    "        return np.mean(vectors)  \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def processing_zh(sentence):\n",
    "    seg_list = jieba.lcut(sentence,cut_all=True)\n",
    "    doc = [word for word in seg_list if word not in stop_words]\n",
    "    docs = [e for e in doc if e.isalnum()]\n",
    "    return docs\n",
    "\n",
    "\n",
    "def get_sentence_embeddings_zh(f):\n",
    "    file = open(f) \n",
    "    lines = file.readlines() \n",
    "    sentences_vectors =[]\n",
    "    for l in lines:\n",
    "        sent  = processing_zh(l)\n",
    "        vec = get_sentence_vector_zh(sent)\n",
    "\n",
    "        if vec is not None:\n",
    "            sentences_vectors.append(vec)\n",
    "        else:\n",
    "            print(l)\n",
    "    return sentences_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82
    },
    "colab_type": "code",
    "id": "6zVjor64tR8D",
    "outputId": "855d213e-b1be-4387-db13-e0a5dfa3074a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.997 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "\n",
    "\n",
    "zh_train_mt = get_sentence_embeddings_zh(\"./train.enzh.mt\")\n",
    "zh_train_src = get_embeddings(\"./train.enzh.src\",glove,nlp_en)\n",
    "f_train_scores = open(\"./train.enzh.scores\",'r')\n",
    "zh_train_scores = f_train_scores.readlines()\n",
    "\n",
    "\n",
    "zh_val_src = get_embeddings(\"./dev.enzh.src\",glove,nlp_en)\n",
    "zh_val_mt = get_sentence_embeddings_zh(\"./dev.enzh.mt\")\n",
    "f_val_scores = open(\"./dev.enzh.scores\",'r')\n",
    "zh_val_scores = f_val_scores.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 65
    },
    "colab_type": "code",
    "id": "N-pXbaJzExaE",
    "outputId": "09f3544c-17bf-4b60-ecc1-17845d5ac8ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mt: 7000 Training src: 7000\n",
      "\n",
      "Validation mt: 1000 Validation src: 1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training mt: {len(zh_train_mt)} Training src: {len(zh_train_src)}\")\n",
    "print()\n",
    "print(f\"Validation mt: {len(zh_val_mt)} Validation src: {len(zh_val_src)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ljBHJpa9ATNf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "X_train= [np.array(zh_train_src),np.array(zh_train_mt)]\n",
    "X_train_zh = np.array(X_train).transpose()\n",
    "\n",
    "X_val = [np.array(zh_val_src),np.array(zh_val_mt)]\n",
    "X_val_zh = np.array(X_val).transpose()\n",
    "\n",
    "#Scores\n",
    "train_scores = np.array(zh_train_scores).astype(float)\n",
    "y_train_zh =train_scores\n",
    "\n",
    "val_scores = np.array(zh_val_scores).astype(float)\n",
    "y_val_zh =val_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "luNEyefram7Z"
   },
   "source": [
    "### Training the Regressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hvCS6pa-yIIB"
   },
   "source": [
    "At this point,  will try SVM and Random Tree Forests and choose the model with the highest Pearson correlation.\n",
    "\n",
    "First we will define our RMSE function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USalvKtRAvQv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a0wOEUhXgteG"
   },
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9rY29AeVyM1n"
   },
   "source": [
    "SVM have many parameters such as the kernel and the regularizating constant C. Here we will use default C = 1 and compare kernels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "Bf_aJK0QK8jx",
    "outputId": "54c20ecc-044a-40cf-c8ab-cd6fd3dced60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear\n",
      "RMSE: 0.9569251357826374 Pearson 0.078828752924285\n",
      "\n",
      "poly\n",
      "RMSE: 0.9573340665924616 Pearson 0.06684940857693276\n",
      "\n",
      "rbf\n",
      "RMSE: 0.9515605754342689 Pearson 0.11468776355641784\n",
      "\n",
      "sigmoid\n",
      "RMSE: 318.5385411750097 Pearson 0.061956801685699746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "for k in ['linear','poly','rbf','sigmoid']:\n",
    "    clf_t = SVR(kernel=k)\n",
    "    clf_t.fit(X_train_zh, y_train_zh)\n",
    "    print(k)\n",
    "    predictions = clf_t.predict(X_val_zh)\n",
    "    pearson = pearsonr(y_val_zh, predictions)\n",
    "    print(f'RMSE: {rmse(predictions,y_val_zh)} Pearson {pearson[0]}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yaOP4zpOgkFP"
   },
   "source": [
    "In this case, the radial basis function kernel performed the best with a Pearson correlation of 0.1147. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wtg69eGbgmHI"
   },
   "source": [
    "#### Random Tree Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OD22DmWvyPs-"
   },
   "source": [
    "Another powerful regressor is the Random Tree Forest. Here we have to choose the number of trees we want to compute and we will pick n_estimators = 1000. The higher the number the longer it will compute. To fine tune that number you could compute the error per number of trees and select the number for which there is no more significant improvement( the \"elbow\" of the graph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "id": "6wEoExkggqHG",
    "outputId": "2dd2d573-3431-446f-976c-f0bcbeabbaaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9603546068992237\n",
      "Pearson 0.0748575846434103\n"
     ]
    }
   ],
   "source": [
    "# Import the model we are using\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=1000, random_state=666)\n",
    "rf.fit(X_train_zh, y_train_zh);\n",
    "predictions = rf.predict(X_val_zh)\n",
    "\n",
    "pearson = pearsonr(y_val_zh, predictions)\n",
    "print('RMSE:', rmse(predictions,y_val_zh))\n",
    "print(f\"Pearson {pearson[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QWiQ2X6Lj3iG"
   },
   "source": [
    "Finally, we see that SVM with RBF kernel is the best model here. We will now use it to predict on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cuIsX8LNiOJm"
   },
   "source": [
    "### Writing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5SQlcfiCITuC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def writeScores(method_name,scores):\n",
    "    fn = \"predictions.txt\"\n",
    "    print(\"\")\n",
    "    with open(fn, 'w') as output_file:\n",
    "        for idx,x in enumerate(scores):\n",
    "            #out =  metrics[idx]+\":\"+str(\"{0:.2f}\".format(x))+\"\\n\"\n",
    "            #print(out)\n",
    "            output_file.write(f\"{x}\\n\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VC3ALWVEXYVi"
   },
   "outputs": [],
   "source": [
    "#EN_ZH\n",
    "\n",
    "zh_test_mt = get_sentence_embeddings_zh(\"./test.enzh.mt\")\n",
    "zh_test_src = get_embeddings(\"./test.enzh.src\",glove,nlp_en)\n",
    "\n",
    "X= [np.array(zh_test_mt),np.array(zh_test_src)]\n",
    "X_test_zh = np.array(X).transpose()\n",
    "\n",
    "#Predict\n",
    "clf_zh = SVR(kernel='rbf')\n",
    "clf_zh.fit(X_train_zh, y_train_zh)\n",
    "\n",
    "predictions_zh = clf_zh.predict(X_test_zh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "id": "r-nDAsi3Xt-4",
    "outputId": "48dbd0e1-1929-4a84-dd59-716e52eab303"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#EN_ZH\n",
    "\n",
    "from google.colab import files\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "writeScores(\"SVR\",predictions_zh)\n",
    "\n",
    "with ZipFile(\"en-zh_svr.zip\",\"w\") as newzip:\n",
    "    newzip.write(\"predictions.txt\")\n",
    " \n",
    "files.download('en-zh_svr.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QlGblmKPyUFr"
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qWUSJ7kAyXxC"
   },
   "source": [
    "Once submitted to codalab, the pearson correlation is 0.0795"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "baseline.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
