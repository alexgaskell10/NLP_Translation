{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "torch.set_printoptions(4)\n",
    "from torch import optim\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "#model = BertModel.from_pretrained('..\\..\\Dump Files\\models\\BERT_cased_multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "\n",
    "def load_data(set_name):\n",
    "    \"\"\"\n",
    "    set name: \"train\", \"dev\", \"test\"\n",
    "    \"\"\"\n",
    "\n",
    "# Load train data into variables\n",
    "    with open(os.path.join(\"..\", \"data\", \"{}.ende.src\".format(set_name)), \"r\", encoding=\"utf8\") as ende_src:\n",
    "        en_set = ende_src.read().split('\\n')\n",
    "    with open(os.path.join(\"..\", \"data\", \"{}.ende.mt\".format(set_name)), \"r\", encoding=\"utf8\") as ende_mt:\n",
    "        de_set = ende_mt.read().split('\\n')\n",
    "    \n",
    "\n",
    "    del en_set[len(en_set)-1]\n",
    "    del de_set[len(de_set)-1]\n",
    "\n",
    "    \n",
    "    return en_set, de_set\n",
    "\n",
    "def load_scores(set_name):\n",
    "    \n",
    "    with open(os.path.join(\"..\", \"data\", \"{}.ende.scores\".format(set_name)), \"r\", encoding=\"utf8\") as ende_scores:\n",
    "        scores = [float(x) for x in ende_scores.read().split('\\n')[:-1]]\n",
    "        \n",
    "    #del scores[len(scores)-1]\n",
    "    print(len(scores))\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "en_train, de_train = load_data(\"train\")\n",
    "scores_train = load_scores(\"train\")\n",
    "\n",
    "en_val, de_val = load_data(\"dev\")\n",
    "scores_val = load_scores(\"dev\")\n",
    "\n",
    "en_test, de_test = load_data(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X, tokenizer):\n",
    "    inputs = []\n",
    "    max_len_inps = 0\n",
    "\n",
    "    # Tokenize\n",
    "    for i in range(len(X)-1):\n",
    "        seq = X[i][:-1]\n",
    "        # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n",
    "        input_ids = torch.tensor([tokenizer.encode(seq, add_special_tokens=True)])  \n",
    "        inputs.append(input_ids)\n",
    "        if input_ids.shape[-1] > max_len_inps:\n",
    "            max_len_inps = input_ids.shape[-1]\n",
    "            \n",
    "    # Convert to tensor\n",
    "    inp_tensor = torch.zeros((len(X), max_len_inps))\n",
    "    \n",
    "    for i in range(len(inputs)):\n",
    "    # Add tokens\n",
    "        tokens = inputs[i].squeeze()\n",
    "        inp_tensor[i, : len(tokens)] = tokens\n",
    "        \n",
    "    print(inp_tensor.shape)\n",
    "    return inp_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7000, 65])\n",
      "torch.Size([7000, 69])\n"
     ]
    }
   ],
   "source": [
    "en_inputs = preprocess(en_train, tokenizer)\n",
    "de_inputs = preprocess(de_train, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7000, 69])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_inputs_new = torch.zeros(de_inputs.shape)\n",
    "en_inputs_new[:,:65] = en_inputs\n",
    "en_inputs_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 48])\n",
      "torch.Size([1000, 54])\n",
      "torch.Size([1000, 69])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 69])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_val_inputs = preprocess(en_val, tokenizer)\n",
    "de_val_inputs = preprocess(de_val, tokenizer)\n",
    "\n",
    "en_val_inputs_new = torch.zeros((1000,de_inputs.shape[1]))\n",
    "en_val_inputs_new[:,:48] = en_val_inputs\n",
    "print(en_val_inputs_new.shape)\n",
    "\n",
    "de_val_inputs_new = torch.zeros((1000, de_inputs.shape[1]))\n",
    "de_val_inputs_new[:,:54] = de_val_inputs\n",
    "de_val_inputs_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_mask(list_input_tensor):\n",
    "    list_attention_masks = []\n",
    "    for input_tensor in list_input_tensor:\n",
    "        attention_mask = torch.zeros((input_tensor.shape))\n",
    "        attention_mask[input_tensor != 0] = 1\n",
    "        list_attention_masks.append(attention_mask)\n",
    "\n",
    "    return list_attention_masks\n",
    "\n",
    "def get_batches(inp_tensor, scores, BATCH_N):\n",
    "    inp_tensor = inp_tensor\n",
    "    scores = torch.tensor(scores).view(-1,1)\n",
    "\n",
    "    # Split data into train, test and val batches\n",
    "    inp_tensor_batches = torch.split(inp_tensor, BATCH_N)\n",
    "    scores_batches = torch.split(scores, BATCH_N)\n",
    "#     X_train_val, X_test, y_train_val, y_test = train_test_split(inp_tensor_batches, scores_batches, shuffle=False, test_size=0.1)\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, shuffle = False, test_size=1/9)\n",
    "    \n",
    "    \n",
    "    #print(len(inp_tensor_batches), inp_tensor_batches[0].shape)\n",
    "\n",
    "    # Create attention masks\n",
    "    X_mask = attention_mask(inp_tensor_batches)\n",
    "    #print(len(X_mask), X_mask[0].shape)\n",
    "\n",
    "    # Batch X, mask and y together\n",
    "    batches = [(X, mask, y) for X,mask,y in zip(inp_tensor_batches, X_mask, scores_batches)]\n",
    "\n",
    "\n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embeddings(batches):\n",
    "    with torch.no_grad():\n",
    "        model = model = BertModel.from_pretrained('..\\..\\Dump Files\\models\\BERT_cased_multi').to(device='cuda')\n",
    "        #inp_tensor = inp_tensor.to(device='cuda', dtype=torch.long)\n",
    "\n",
    "\n",
    "        #batches = torch.split(inp_tensor, 250, dim=0)\n",
    "        list_bert_embs = []\n",
    "        for X in batches:\n",
    "            with torch.no_grad():\n",
    "                last_hidden_states = model(X[0].type(torch.LongTensor).to('cuda'), X[1].to('cuda'))[0]    # <-- take word embeddings ([1] gives sentence embeddings)\n",
    "\n",
    "            list_bert_embs.append(last_hidden_states)\n",
    "\n",
    "            #print(last_hidden_states.shape)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        bert_embs = torch.cat(list_bert_embs, dim=0)\n",
    "\n",
    "        # Now, slice the tensor to only keep the [CLS] embedding\n",
    "\n",
    "        print(bert_embs.shape)\n",
    "    \n",
    "    return bert_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "#bert_config = 'bert-base-multilingual-cased'\n",
    "BATCH_N = 25\n",
    "EPOCHS = 20\n",
    "LR = 2e-6\n",
    "\n",
    "# Set seeds\n",
    "seed_val = 111\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Preprocessing\n",
    "\n",
    "\n",
    "batches_en_train = get_batches(en_inputs_new, scores_train, BATCH_N)\n",
    "batches_de_train = get_batches(de_inputs, scores_train, BATCH_N)\n",
    "\n",
    "batches_en_val = get_batches(en_val_inputs_new, scores_val, BATCH_N)\n",
    "batches_de_val = get_batches(de_val_inputs_new, scores_val, BATCH_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7000, 69, 768])\n"
     ]
    }
   ],
   "source": [
    "embs_en_train = get_sentence_embeddings(batches_en_train).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7000, 69, 768])\n"
     ]
    }
   ],
   "source": [
    "embs_de_train = get_sentence_embeddings(batches_de_train).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 69, 768])\n",
      "torch.Size([1000, 69, 768])\n"
     ]
    }
   ],
   "source": [
    "embs_en_val = get_sentence_embeddings(batches_en_val).to('cpu')\n",
    "embs_de_val = get_sentence_embeddings(batches_de_val).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7000, 2, 69, 768])\n",
      "torch.Size([1000, 2, 69, 768])\n"
     ]
    }
   ],
   "source": [
    "embs_en_train = embs_en_train.view(7000,1,69,768)\n",
    "embs_de_train = embs_de_train.view(7000,1,69,768)\n",
    "train_dataset = torch.cat((embs_en_train,embs_de_train), 1)\n",
    "print(train_dataset.shape)\n",
    "\n",
    "embs_en_val = embs_en_val.view(1000,1,69,768)\n",
    "embs_de_val = embs_de_val.view(1000,1,69,768)\n",
    "val_dataset = torch.cat((embs_en_val,embs_de_val), 1)\n",
    "print(val_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert scores to tensor\n",
    "torch_scores_train = torch.tensor(scores_train).view(-1,1)\n",
    "torch_scores_val = torch.tensor(scores_val).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make final dataset\n",
    "train = (train_dataset.type(torch.LongTensor), torch_scores_train.type(torch.LongTensor))\n",
    "val = (val_dataset.type(torch.LongTensor), torch_scores_val.type(torch.LongTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_batches = torch.split(train_dataset, BATCH_N)\n",
    "train_score_batches = torch.split(torch_scores_train, BATCH_N)\n",
    "train_batches = [(X, y) for X,y in zip(train_data_batches, train_score_batches)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_batches = torch.split(val_dataset, BATCH_N)\n",
    "val_score_batches = torch.split(torch_scores_val, BATCH_N)\n",
    "val_batches = [(X, y) for X,y in zip(val_data_batches, val_score_batches)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batches[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del val_data_batches, val_score_batches, train_data_batches, train_score_batches, train, val, embs_en_train, embs_de_train, embs_en_val, embs_de_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define resnet building blocks\n",
    "\n",
    "class ResidualBlock(nn.Module): \n",
    "    def __init__(self, inchannel, outchannel, stride=1): \n",
    "        \n",
    "        super(ResidualBlock, self).__init__() \n",
    "        \n",
    "        self.left = nn.Sequential(nn.Conv2d(inchannel, outchannel, kernel_size=3, \n",
    "                                         stride=stride, padding=1, bias=False), \n",
    "                                  nn.BatchNorm2d(outchannel), \n",
    "                                  nn.ReLU(inplace=True), \n",
    "                                  nn.Conv2d(outchannel, outchannel, kernel_size=3, \n",
    "                                         stride=1, padding=1, bias=False), \n",
    "                                  nn.BatchNorm2d(outchannel)) \n",
    "        \n",
    "        self.shortcut = nn.Sequential() \n",
    "        \n",
    "        if stride != 1 or inchannel != outchannel: \n",
    "            \n",
    "            self.shortcut = nn.Sequential(nn.Conv2d(inchannel, outchannel, \n",
    "                                                 kernel_size=1, stride=stride, \n",
    "                                                 padding = 0, bias=False), \n",
    "                                          nn.BatchNorm2d(outchannel) ) \n",
    "            \n",
    "    def forward(self, x): \n",
    "        \n",
    "        out = self.left(x) \n",
    "        \n",
    "        out += self.shortcut(x) \n",
    "        \n",
    "        out = F.relu(out) \n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "    \n",
    "    # define resnet\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, ResidualBlock, num_classes = 10):\n",
    "        \n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.inchannel = 64\n",
    "        #self.bn1 = self.BatchNorm2d(2)\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(2, 64, kernel_size = (5,768), stride = 1, # changed 3 in channels to 2\n",
    "                                            padding = 1, bias = False), \n",
    "                                  nn.BatchNorm2d(64), \n",
    "                                  nn.ReLU())\n",
    "        \n",
    "        self.layer1 = self.make_layer(ResidualBlock, 64, 2, stride = 1)\n",
    "        self.layer2 = self.make_layer(ResidualBlock, 128, 2, stride = 2)\n",
    "        self.layer3 = self.make_layer(ResidualBlock, 256, 2, stride = 2)\n",
    "        self.layer4 = self.make_layer(ResidualBlock, 512, 2, stride = 2)\n",
    "        self.maxpool = nn.MaxPool2d(4)\n",
    "        self.fc = nn.Linear(4608, 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def make_layer(self, block, channels, num_blocks, stride):\n",
    "        \n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        for stride in strides:\n",
    "            \n",
    "            layers.append(block(self.inchannel, channels, stride))\n",
    "            \n",
    "            self.inchannel = channels\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.layer1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.layer2(x)\n",
    "        #print(x.shape)\n",
    "        x = self.layer3(x)\n",
    "        #print(x.shape)\n",
    "        x = self.layer4(x)\n",
    "        #print(x.shape)\n",
    "        #x = self.maxpool(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def loss(self, scores, pred_scores):\n",
    "        rmse = (((pred_scores - scores)**2).mean())**0.5\n",
    "        return rmse\n",
    "\n",
    "    def check_r(self, y_pred, y):\n",
    "        return pearsonr(y_pred.cpu().squeeze(), y.cpu().squeeze())[0]\n",
    "    \n",
    "    \n",
    "def ResNet18():\n",
    "    return ResNet(ResidualBlock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Manual seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Define two train sets: one with normal images and the other with\n",
    "# images horizontally flipped\n",
    "\n",
    "\n",
    "# Train dataloader\n",
    "# Concatenate two datasets two increase the size of training set artificially by 2\n",
    "# This data augmentation is permitted as per Piazza @47\n",
    "# loader_train = torch.utils.data.DataLoader(\n",
    "#     train, \n",
    "#     batch_size=batch_size, shuffle=True, num_workers=10\n",
    "#                                            )\n",
    "# # Test dataloader\n",
    "# loader_val = torch.utils.data.DataLoader(val, batch_size=batch_size,\n",
    "#                                          shuffle=False, num_workers=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, val_batches):\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    eval_loss = 0\n",
    "    correlations = []\n",
    "\n",
    "    for step, batch in enumerate(val_batches):        \n",
    "\n",
    "        # Untie batch and put on GPU\n",
    "#         X = batch[0].to(device=device, dtype=torch.long)\n",
    "#         mask = batch[1].to(device)\n",
    "#         y = batch[2].to(device)\n",
    "        X = batch[0].to('cuda')\n",
    "        y = batch[1].to('cuda')\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            y_pred = model(X)                      \n",
    "                \n",
    "        # Compute and record batch accuracy\n",
    "        corr = model.check_r(y_pred, y)\n",
    "        correlations.append(corr)\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(f\"|  Correlation: {np.mean(correlations):.2f}     |\")\n",
    "    print(f\"|  Validation took: {time.time() - t0:0f} s |\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def train(model, train_batches, val_batches, epochs):\n",
    "\n",
    "    losses = []\n",
    "    \n",
    "    for epoch_i in range(epochs):\n",
    "        \n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        t0 = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "        for step, batch in enumerate(train_batches):\n",
    "\n",
    "#             # Untie batch and put on GPU\n",
    "#             X = X.to(device=device)\n",
    "#             #mask = batch[1].to(device)\n",
    "#             y = y.to(device)\n",
    "            \n",
    "            X = batch[0].to('cuda')\n",
    "            y = batch[1].to('cuda')\n",
    "    \n",
    "            model.zero_grad()                   # Reset grads\n",
    "            y_pred = model(X)             # Forward pass\n",
    "            loss = model.loss(y, y_pred)        # Compute loss\n",
    "            total_loss += loss.item()           # Accumulate loss\n",
    "            loss.backward()                     # Backward pass\n",
    "\n",
    "            optimizer.step()                    # Update params\n",
    "\n",
    "            if step % 140 == 0:\n",
    "                # Progress update every 40 batches                \n",
    "                print('  Batch {:>5,}  of  {:>5,}    |    Elapsed: {:.0f}s.'.format(step, len(train_batches), time.time() - t0))\n",
    "                print(f'  Loss = {loss.item():.2f}')\n",
    "\n",
    "            torch.cuda.empty_cache()            # Clear GPU cache to avoid memory issues\n",
    "\n",
    "        # Compute and store avg loss\n",
    "        avg_train_loss = total_loss / len(X)\n",
    "        losses.append(avg_train_loss)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:.0f}s\".format(time.time() - t0))\n",
    "\n",
    "        validation(model, val_batches) \n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def writeScores(method_name, scores):\n",
    "    fn = \"predictions.txt\"\n",
    "    print(\"\")\n",
    "    with open(fn, 'w') as output_file:\n",
    "        for idx,x in enumerate(scores):\n",
    "            out =  metrics[idx]+\":\"+str(\"{0:.2f}\".format(x))+\"\\n\"\n",
    "            print(out)\n",
    "            # output_file.write(f\"{x}\\n\")\n",
    "\n",
    "\n",
    "def get_test_preds(model, en_test, de_test):\n",
    "    torch.cuda.empty_cache()\n",
    "    # Preprocessing\n",
    "    preprocessor = Preprocessor(device, bert_config)\n",
    "    input_tensor = preprocessor.get_input_tensor(en_test, de_test)\n",
    "    batches = preprocessor.get_test_batches(input_tensor, BATCH_N=32)\n",
    "    # mask = torch.cat(preprocessor.attention_mask(input_tensor), dim=0).view(input_tensor.shape).to(device).long()\n",
    "    # print(mask.shape)\n",
    "    # print(input_tensor.shape)\n",
    "    y_preds = []\n",
    "    for batch in batches:\n",
    "        # Untie batch and put on GPU\n",
    "        X = batch[0].to(device=device, dtype=torch.long)\n",
    "        mask = batch[1].to(device)\n",
    "\n",
    "        model.zero_grad()                   # Reset grads\n",
    "        y_pred = model(X, mask)             # Forward pass\n",
    "\n",
    "        y_preds.append(y_pred)\n",
    "\n",
    "    print(y_preds)\n",
    "    # writeScores(y_pred)\n",
    "    # torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet=ResNet18().to('cuda')\n",
    "optimizer = optim.Adam(resnet.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 1.42\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.96\n",
      "\n",
      "  Average training loss: 11.25\n",
      "  Training epoch took: 10s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: -0.06     |\n",
      "|  Validation took: 0.327296 s |\n",
      "\n",
      "======== Epoch 2 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 1.56\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.65\n",
      "\n",
      "  Average training loss: 9.95\n",
      "  Training epoch took: 10s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.07     |\n",
      "|  Validation took: 0.325295 s |\n",
      "\n",
      "======== Epoch 3 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 1.51\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.88\n",
      "\n",
      "  Average training loss: 9.49\n",
      "  Training epoch took: 10s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: -0.01     |\n",
      "|  Validation took: 0.326296 s |\n",
      "\n",
      "======== Epoch 4 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 1.69\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.69\n",
      "\n",
      "  Average training loss: 9.63\n",
      "  Training epoch took: 10s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.01     |\n",
      "|  Validation took: 0.329299 s |\n",
      "\n",
      "======== Epoch 5 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 1.21\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.64\n",
      "\n",
      "  Average training loss: 9.34\n",
      "  Training epoch took: 10s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.10     |\n",
      "|  Validation took: 0.324294 s |\n",
      "\n",
      "======== Epoch 6 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 1.30\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.69\n",
      "\n",
      "  Average training loss: 9.01\n",
      "  Training epoch took: 10s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.13     |\n",
      "|  Validation took: 0.323294 s |\n",
      "\n",
      "======== Epoch 7 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 1.48\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.65\n",
      "\n",
      "  Average training loss: 8.63\n",
      "  Training epoch took: 10s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.03     |\n",
      "|  Validation took: 0.327298 s |\n",
      "\n",
      "======== Epoch 8 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 1.67\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.88\n",
      "\n",
      "  Average training loss: 8.85\n",
      "  Training epoch took: 10s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.05     |\n",
      "|  Validation took: 0.324295 s |\n",
      "\n",
      "======== Epoch 9 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 1.82\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.63\n",
      "\n",
      "  Average training loss: 8.19\n",
      "  Training epoch took: 10s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.02     |\n",
      "|  Validation took: 0.325296 s |\n",
      "\n",
      "======== Epoch 10 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 1.67\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.65\n",
      "\n",
      "  Average training loss: 7.75\n",
      "  Training epoch took: 10s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.05     |\n",
      "|  Validation took: 0.324295 s |\n",
      "\n",
      "======== Epoch 11 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 1.57\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.55\n",
      "\n",
      "  Average training loss: 7.05\n",
      "  Training epoch took: 10s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.05     |\n",
      "|  Validation took: 0.326296 s |\n",
      "\n",
      "======== Epoch 12 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 1.44\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.64\n",
      "\n",
      "  Average training loss: 6.80\n",
      "  Training epoch took: 10s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.07     |\n",
      "|  Validation took: 0.327297 s |\n",
      "\n",
      "======== Epoch 13 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 1.38\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.65\n",
      "\n",
      "  Average training loss: 6.33\n",
      "  Training epoch took: 10s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.06     |\n",
      "|  Validation took: 0.326297 s |\n",
      "\n",
      "======== Epoch 14 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 1.26\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.47\n",
      "\n",
      "  Average training loss: 5.90\n",
      "  Training epoch took: 10s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.07     |\n",
      "|  Validation took: 0.328299 s |\n",
      "\n",
      "======== Epoch 15 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 1.21\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.48\n",
      "\n",
      "  Average training loss: 5.59\n",
      "  Training epoch took: 10s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.07     |\n",
      "|  Validation took: 0.338307 s |\n",
      "\n",
      "======== Epoch 16 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 1.00\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.47\n",
      "\n",
      "  Average training loss: 5.10\n",
      "  Training epoch took: 10s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.06     |\n",
      "|  Validation took: 0.328299 s |\n",
      "\n",
      "======== Epoch 17 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 0.96\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.45\n",
      "\n",
      "  Average training loss: 4.91\n",
      "  Training epoch took: 10s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.04     |\n",
      "|  Validation took: 0.325295 s |\n",
      "\n",
      "======== Epoch 18 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 0.81\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.40\n",
      "\n",
      "  Average training loss: 4.57\n",
      "  Training epoch took: 10s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.08     |\n",
      "|  Validation took: 0.335305 s |\n",
      "\n",
      "======== Epoch 19 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 0.74\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.31\n",
      "\n",
      "  Average training loss: 4.19\n",
      "  Training epoch took: 10s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.05     |\n",
      "|  Validation took: 0.324295 s |\n",
      "\n",
      "======== Epoch 20 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 0.69\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.36\n",
      "\n",
      "  Average training loss: 3.98\n",
      "  Training epoch took: 10s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.09     |\n",
      "|  Validation took: 0.327298 s |\n",
      "\n",
      "======== Epoch 21 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 0.62\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.30\n",
      "\n",
      "  Average training loss: 3.82\n",
      "  Training epoch took: 10s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.08     |\n",
      "|  Validation took: 0.326297 s |\n",
      "\n",
      "======== Epoch 22 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 0.66\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.28\n",
      "\n",
      "  Average training loss: 3.67\n",
      "  Training epoch took: 10s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.08     |\n",
      "|  Validation took: 0.326297 s |\n",
      "\n",
      "======== Epoch 23 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 0.53\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.28\n",
      "\n",
      "  Average training loss: 3.39\n",
      "  Training epoch took: 10s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.12     |\n",
      "|  Validation took: 0.327298 s |\n",
      "\n",
      "======== Epoch 24 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 0.44\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.28\n",
      "\n",
      "  Average training loss: 3.08\n",
      "  Training epoch took: 10s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.10     |\n",
      "|  Validation took: 0.326297 s |\n",
      "\n",
      "======== Epoch 25 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 0.43\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.26\n",
      "\n",
      "  Average training loss: 2.99\n",
      "  Training epoch took: 10s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.09     |\n",
      "|  Validation took: 0.322285 s |\n",
      "\n",
      "======== Epoch 26 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 0.40\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.41\n",
      "\n",
      "  Average training loss: 2.89\n",
      "  Training epoch took: 10s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.09     |\n",
      "|  Validation took: 0.324294 s |\n",
      "\n",
      "======== Epoch 27 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 0.43\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.25\n",
      "\n",
      "  Average training loss: 2.86\n",
      "  Training epoch took: 10s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.09     |\n",
      "|  Validation took: 0.323294 s |\n",
      "\n",
      "======== Epoch 28 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 0.53\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.25\n",
      "\n",
      "  Average training loss: 2.58\n",
      "  Training epoch took: 10s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.08     |\n",
      "|  Validation took: 0.324295 s |\n",
      "\n",
      "======== Epoch 29 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 0.59\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.22\n",
      "\n",
      "  Average training loss: 2.55\n",
      "  Training epoch took: 10s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.09     |\n",
      "|  Validation took: 0.330300 s |\n",
      "\n",
      "======== Epoch 30 / 60 ========\n",
      "Training...\n",
      "  Batch     0  of    280    |    Elapsed: 0s.\n",
      "  Loss = 0.30\n",
      "  Batch   140  of    280    |    Elapsed: 5s.\n",
      "  Loss = 0.25\n"
     ]
    }
   ],
   "source": [
    "train(resnet, train_batches, val_batches, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next(iter(loader_train))\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-a7ab210f85a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37-msc",
   "language": "python",
   "name": "py37-msc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
