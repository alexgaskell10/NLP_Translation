{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"py37-msc","language":"python","name":"py37-msc"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Mikes_BERT_CNN.ipynb","provenance":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"va_SuInjVa2Z","colab_type":"text"},"source":["# NLP Coursework: CNNs with BERT"]},{"cell_type":"markdown","metadata":{"id":"T-DYW1_vVhSW","colab_type":"text"},"source":["### Imports and data/model loading\n"]},{"cell_type":"code","metadata":{"id":"TWHFO0zcLP5S","colab_type":"code","outputId":"84a4a73d-15b6-492a-ce85-e90aa4f19dd3","executionInfo":{"status":"ok","timestamp":1582669831345,"user_tz":0,"elapsed":6121,"user":{"displayName":"Michael Zotov","photoUrl":"","userId":"18245138602260748773"}},"colab":{"base_uri":"https://localhost:8080/","height":417}},"source":["! pip install transformers"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n","Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n","Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4--BOqs2L9NX","colab_type":"code","outputId":"6d1d539f-8b0d-4fa2-b2af-97cf89032c04","executionInfo":{"status":"ok","timestamp":1582669832830,"user_tz":0,"elapsed":7476,"user":{"displayName":"Michael Zotov","photoUrl":"","userId":"18245138602260748773"}},"colab":{"base_uri":"https://localhost:8080/","height":63}},"source":["import torch\n","from torch import optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","# Transformer library for pre-trained BERT\n","from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n","\n","\n","from sklearn.model_selection import train_test_split\n","from scipy.stats import pearsonr\n","\n","import io\n","import os\n","import random\n","import time\n","from tqdm import tqdm\n","\n","# Set printing\n","torch.set_printoptions(4)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"ICmh_uaxLxvu","colab_type":"code","colab":{}},"source":["if not os.path.exists('ende_data.zip'):\n","    !wget -O ende_data.zip https://competitions.codalab.org/my/datasets/download/c748d2c0-d6be-4e36-9f12-ca0e88819c4d\n","    !unzip ende_data.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a4AQ8BUtLP5X","colab_type":"code","colab":{}},"source":["# Load the tokenizer\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n","# Load the model\n","model = BertModel.from_pretrained(\"bert-base-multilingual-cased\").to(device='cuda')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sVcPmOBSLP5Z","colab_type":"code","colab":{}},"source":["# A function used for dataloading\n","\n","def load_data(set_name):\n","    \"\"\"\n","    set name: \"train\", \"dev\", \"test\"\n","    \"\"\"\n","\n","# Load data into variables\n","    with open(\"./{}.ende.src\".format(set_name), \"r\") as ende_src:\n","        en_set = ende_src.read().split('\\n')\n","    with open(\"./{}.ende.mt\".format(set_name), \"r\") as ende_mt:\n","        de_set = ende_mt.read().split('\\n')\n","    \n","    # Clear the last row as the function reads 7001/1001 lines\n","    del en_set[len(en_set)-1]\n","    del de_set[len(de_set)-1]\n","\n","    \n","    return en_set, de_set\n","\n","def load_scores(set_name):\n","    \n","    with open(\"./{}.ende.scores\".format(set_name), \"r\") as ende_scores:\n","        scores = [float(x) for x in ende_scores.read().split('\\n')[:-1]]\n","    \n","    return scores"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"shHb1VeOLP5b","colab_type":"code","colab":{}},"source":["# Load all data\n","en_train, de_train = load_data(\"train\")\n","scores_train = load_scores(\"train\")\n","\n","en_val, de_val = load_data(\"dev\")\n","scores_val = load_scores(\"dev\")\n","\n","en_test, de_test = load_data(\"test\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C0X65l6OVoXz","colab_type":"text"},"source":["### Preprocessing and embeddings"]},{"cell_type":"code","metadata":{"id":"Q_IVikqKLP5j","colab_type":"code","colab":{}},"source":["def preprocess(X, tokenizer):\n","    inputs = []\n","    max_len_inps = 0\n","\n","    # Tokenize\n","    for i in range(len(X)-1):\n","        seq = X[i][:-1]\n","        # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n","        input_ids = torch.tensor([tokenizer.encode(seq, add_special_tokens=True)])  \n","        inputs.append(input_ids)\n","        if input_ids.shape[-1] > max_len_inps:\n","            max_len_inps = input_ids.shape[-1]\n","            \n","    # Convert to tensor\n","    inp_tensor = torch.zeros((len(X), max_len_inps))\n","    \n","    for i in range(len(inputs)):\n","    # Add tokens\n","        tokens = inputs[i].squeeze()\n","        inp_tensor[i, : len(tokens)] = tokens\n","        \n","    print(inp_tensor.shape)\n","    return inp_tensor"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jG1rbGqLLP5l","colab_type":"code","outputId":"4947e3e4-b875-46e7-bfb9-d2fc03d99e71","executionInfo":{"status":"ok","timestamp":1582669847609,"user_tz":0,"elapsed":21167,"user":{"displayName":"Michael Zotov","photoUrl":"","userId":"18245138602260748773"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["# Preprocess data\n","en_inputs = preprocess(en_train, tokenizer)\n","de_inputs = preprocess(de_train, tokenizer)\n","\n","en_val_inputs = preprocess(en_val, tokenizer)\n","de_val_inputs = preprocess(de_val, tokenizer)\n","\n","en_test_inputs = preprocess(en_test, tokenizer)\n","de_test_inputs = preprocess(de_test, tokenizer)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["torch.Size([7000, 65])\n","torch.Size([7000, 69])\n","torch.Size([1000, 48])\n","torch.Size([1000, 54])\n","torch.Size([1000, 44])\n","torch.Size([1000, 73])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5-CCHRiILP5n","colab_type":"code","colab":{}},"source":["# Pad all data with zeros to bring it to the same size\n","en_inputs_new = torch.zeros((7000, de_test_inputs.shape[1]))\n","en_inputs_new[:,:65] = en_inputs\n","\n","de_inputs_new = torch.zeros((7000, de_test_inputs.shape[1]))\n","de_inputs_new[:,:69] = de_inputs\n","\n","en_val_inputs_new = torch.zeros((1000,de_test_inputs.shape[1]))\n","en_val_inputs_new[:,:48] = en_val_inputs\n","\n","de_val_inputs_new = torch.zeros((1000, de_test_inputs.shape[1]))\n","de_val_inputs_new[:,:54] = de_val_inputs\n","\n","en_test_inputs_new = torch.zeros((1000,de_test_inputs.shape[1]))\n","en_test_inputs_new[:,:44] = en_test_inputs\n","\n","de_test_inputs_new = de_test_inputs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JLwp4qAiLP54","colab_type":"code","colab":{}},"source":["# Produces attention mask for BERT, which is need to communicate the padding locations to it\n","def attention_mask(list_input_tensor):\n","    list_attention_masks = []\n","    for input_tensor in list_input_tensor:\n","        attention_mask = torch.zeros((input_tensor.shape))\n","        attention_mask[input_tensor != 0] = 1\n","        list_attention_masks.append(attention_mask)\n","\n","    return list_attention_masks\n","\n","# Function to split data into batches for obtaining embeddings\n","def get_batches(inp_tensor, scores, BATCH_N):\n","    inp_tensor = inp_tensor\n","    scores = torch.tensor(scores).view(-1,1)\n","\n","    # Split data into train, test and val batches\n","    inp_tensor_batches = torch.split(inp_tensor, BATCH_N)\n","    scores_batches = torch.split(scores, BATCH_N)\n","\n","    # Create attention masks\n","    X_mask = attention_mask(inp_tensor_batches)\n","\n","    # Batch X, mask and y together\n","    batches = [(X, mask, y) for X,mask,y in zip(inp_tensor_batches, X_mask, scores_batches)]\n","\n","    return batches\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SA1BmxchLP56","colab_type":"code","colab":{}},"source":["def get_sentence_embeddings(batches, model):\n","    # This function gets full per-word embeddings as well as [CLS] sentence embedding\n","    with torch.no_grad():\n","        \n","        list_bert_embs = []\n","        for X in batches:\n","            with torch.no_grad():\n","                # Obtain embeddings\n","                last_hidden_states = model(X[0].type(torch.LongTensor).to('cuda'), X[1].to('cuda'))[0]    # <-- take word embeddings ([1] gives sentence embeddings)\n","\n","            list_bert_embs.append(last_hidden_states)\n","\n","\n","            torch.cuda.empty_cache()\n","\n","        bert_embs = torch.cat(list_bert_embs, dim=0)\n","\n","        # Check for correct shape\n","        print(bert_embs.shape)\n","    \n","    return bert_embs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u14aGSYpLP5-","colab_type":"code","colab":{}},"source":["USE_GPU = True\n","\n","if USE_GPU and torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","\n","# Batch Size\n","BATCH_N = 25\n","# Set to few epochs to show training loop. In practice, was trained for longer\n","EPOCHS = 4\n","LR = 0.00001\n","\n","# Set seeds for reproducibility\n","seed_val = 111\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# Split into batches\n","\n","batches_en_train = get_batches(en_inputs_new, scores_train, BATCH_N)\n","batches_de_train = get_batches(de_inputs_new, scores_train, BATCH_N)\n","\n","batches_en_val = get_batches(en_val_inputs_new, scores_val, BATCH_N)\n","batches_de_val = get_batches(de_val_inputs_new, scores_val, BATCH_N)\n","\n","batches_en_test = get_batches(en_test_inputs_new, scores_val, BATCH_N)\n","batches_de_test = get_batches(de_test_inputs_new, scores_val, BATCH_N)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zC3qxwH9LP6A","colab_type":"code","outputId":"699a887b-9a9f-415d-92fc-65ae0b9cfdd5","executionInfo":{"status":"ok","timestamp":1582669891604,"user_tz":0,"elapsed":64358,"user":{"displayName":"Michael Zotov","photoUrl":"","userId":"18245138602260748773"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["embs_en_train = get_sentence_embeddings(batches_en_train, model).to('cpu')\n","embs_de_train = get_sentence_embeddings(batches_de_train, model).to('cpu')\n","embs_en_val = get_sentence_embeddings(batches_en_val, model).to('cpu')\n","embs_de_val = get_sentence_embeddings(batches_de_val, model).to('cpu')\n","embs_en_test = get_sentence_embeddings(batches_en_test, model).to('cpu')\n","embs_de_test = get_sentence_embeddings(batches_de_test, model).to('cpu')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["torch.Size([7000, 73, 768])\n","torch.Size([7000, 73, 768])\n","torch.Size([1000, 73, 768])\n","torch.Size([1000, 73, 768])\n","torch.Size([1000, 73, 768])\n","torch.Size([1000, 73, 768])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4LmfPAEsLP6I","colab_type":"code","outputId":"3e92e2c5-f32c-4108-d77d-dd15b4ad8525","executionInfo":{"status":"ok","timestamp":1582669893599,"user_tz":0,"elapsed":66189,"user":{"displayName":"Michael Zotov","photoUrl":"","userId":"18245138602260748773"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["# Concatenate the embeddings into two channels: 1 channel for English and 1 for German\n","\n","embs_en_train = embs_en_train.view(7000,1,73,768)\n","embs_de_train = embs_de_train.view(7000,1,73,768)\n","train_dataset = torch.cat((embs_en_train,embs_de_train), 1)\n","print(train_dataset.shape)\n","\n","embs_en_val = embs_en_val.view(1000,1,73,768)\n","embs_de_val = embs_de_val.view(1000,1,73,768)\n","val_dataset = torch.cat((embs_en_val,embs_de_val), 1)\n","print(val_dataset.shape)\n","\n","embs_en_test = embs_en_test.view(1000,1,73,768)\n","embs_de_test = embs_de_test.view(1000,1,73,768)\n","test_dataset = torch.cat((embs_en_test, embs_de_test), 1)\n","print(test_dataset.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["torch.Size([7000, 2, 73, 768])\n","torch.Size([1000, 2, 73, 768])\n","torch.Size([1000, 2, 73, 768])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XHQkMxhrLP6K","colab_type":"code","colab":{}},"source":["# Convert scores to tensor\n","torch_scores_train = torch.tensor(scores_train).view(-1,1)\n","torch_scores_val = torch.tensor(scores_val).view(-1,1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2NAVhCOaLP6M","colab_type":"code","colab":{}},"source":["# Build final datasets\n","train = (train_dataset.type(torch.LongTensor), torch_scores_train.type(torch.LongTensor))\n","val = (val_dataset.type(torch.LongTensor), torch_scores_val.type(torch.LongTensor))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PLLVV556LP6P","colab_type":"code","colab":{}},"source":["# Get batches for training\n","train_data_batches = torch.split(train_dataset, BATCH_N)\n","train_score_batches = torch.split(torch_scores_train, BATCH_N)\n","train_batches = [(X, y) for X,y in zip(train_data_batches, train_score_batches)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uX_eGnDMLP6R","colab_type":"code","colab":{}},"source":["# Get validation batches\n","val_data_batches = torch.split(val_dataset, BATCH_N)\n","val_score_batches = torch.split(torch_scores_val, BATCH_N)\n","val_batches = [(X, y) for X,y in zip(val_data_batches, val_score_batches)]\n","\n","# Split validation set into 2 sets: one will be use as validation to save models\n","# The second one will be used as a test set to check generalisation performance\n","# of saved models\n","validation_batches = val_batches[0:20]\n","test_batches = val_batches[20:41]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6g2Z3ET9LP6T","colab_type":"code","colab":{}},"source":["# Clear some RAM\n","del train_dataset, test_dataset, train, val, embs_en_train, embs_en_test, embs_en_val, embs_de_train, embs_de_test, embs_de_val"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CYFUU35AVt6-","colab_type":"text"},"source":["### Models"]},{"cell_type":"code","metadata":{"id":"waOp7ePJLP6Z","colab_type":"code","colab":{}},"source":["# Define resnet building blocks\n","# Architecture modified from CO460 coursework 1\n","\n","class ResidualBlock(nn.Module): \n","    def __init__(self, inchannel, outchannel, stride=1): \n","        \n","        super(ResidualBlock, self).__init__() \n","        \n","        self.left = nn.Sequential(nn.Conv2d(inchannel, outchannel, kernel_size=3, \n","                                         stride=stride, padding=1, bias=False), \n","                                  nn.BatchNorm2d(outchannel), \n","                                  nn.ReLU(inplace=True), \n","                                  nn.Conv2d(outchannel, outchannel, kernel_size=3, \n","                                         stride=1, padding=1, bias=False), \n","                                  nn.BatchNorm2d(outchannel)) \n","        \n","        self.shortcut = nn.Sequential() \n","        \n","        if stride != 1 or inchannel != outchannel: \n","            \n","            self.shortcut = nn.Sequential(nn.Conv2d(inchannel, outchannel, \n","                                                 kernel_size=1, stride=stride, \n","                                                 padding = 0, bias=False), \n","                                          nn.BatchNorm2d(outchannel) ) \n","            \n","    def forward(self, x): \n","        \n","        out = self.left(x) \n","        \n","        out += self.shortcut(x) \n","        \n","        out = F.relu(out) \n","        \n","        return out\n","\n","\n","# Define a narrow ResNet18\n","\n","class ResNet(nn.Module):\n","    \n","    def __init__(self, ResidualBlock):\n","        \n","        super(ResNet, self).__init__()\n","        \n","        self.inchannel = 16\n","        # Encoder layer uses fairly large kernel size due to input size\n","        self.conv1 = nn.Sequential(nn.Conv2d(2, 16, kernel_size = (5,32), stride = 1, \n","                                            padding = 1, bias = False), \n","                                  nn.BatchNorm2d(16), \n","                                  nn.ReLU())\n","        # The rest of the layers are standard ResNet18 architecture but narrower\n","        self.layer1 = self.make_layer(ResidualBlock, 16, 2, stride = 1)\n","        self.layer2 = self.make_layer(ResidualBlock, 32, 2, stride = 2)\n","        self.layer3 = self.make_layer(ResidualBlock, 64, 2, stride = 2)\n","        self.layer4 = self.make_layer(ResidualBlock, 128, 2, stride = 2)\n","        # Aggressive pooling to reduce number of intpus to linear layer\n","        self.maxpool = nn.MaxPool2d((4,16))\n","        # Squeeze output between (-1,1)\n","        self.tanh = nn.Tanh()\n","        self.fc = nn.Linear(1280, 1)\n","        \n","        \n","    def make_layer(self, block, channels, num_blocks, stride):\n","        \n","        strides = [stride] + [1] * (num_blocks - 1)\n","        \n","        layers = []\n","        \n","        for stride in strides:\n","            \n","            layers.append(block(self.inchannel, channels, stride))\n","            \n","            self.inchannel = channels\n","            \n","        return nn.Sequential(*layers)\n","    \n","    \n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","        x = self.maxpool(x)\n","        x = x.view(x.size(0), -1)     \n","        x = self.tanh(x)       \n","        x = self.fc(x)\n","        \n","        return x\n","    \n","    def check_r(self, y_pred, y):\n","        # Compute Pearson Correlation Coefficient\n","        return pearsonr(y_pred.cpu().squeeze(), y.cpu().squeeze())[0]\n","    \n","    def loss(self, scores, pred_scores):\n","        # RMSE loss\n","        loss = (((pred_scores - scores)**2).mean())**0.5\n","        return loss\n","\n","def ResNet18():\n","    return ResNet(ResidualBlock)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UJ2U4p6jLP6e","colab_type":"text"},"source":["### ConvNet"]},{"cell_type":"code","metadata":{"id":"z0dYY0YcLP6e","colab_type":"code","colab":{}},"source":["# Define a simple CNN\n","class ConvNet(nn.Module):\n","    \n","    def __init__(self):\n","        \n","        super(ConvNet, self).__init__()\n","        \n","        self.inchannel = 16\n","        # Very large convolutional window on encoder\n","        self.conv = nn.Sequential(nn.Conv2d(2, 16, kernel_size = (50,300), stride = 1, \n","                                            padding = 1, bias = False), \n","                                  nn.BatchNorm2d(16), \n","                                  nn.ReLU(), \n","                                   nn.Conv2d(16, 16, kernel_size = (5,5), stride = 1, \n","                                            padding = 1, bias = False), \n","                                  nn.BatchNorm2d(16), \n","                                  nn.ReLU())\n","        \n","        self.maxpool = nn.MaxPool2d((8,32))\n","\n","        self.tanh = nn.Tanh()\n","        self.fc = nn.Linear(672, 1)\n","        \n","        \n","    \n","    \n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.maxpool(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.tanh(x)\n","        x = self.fc(x)\n","        \n","        return x\n","    \n","    def check_r(self, y_pred, y):\n","        # Compute Pearson Correlation Coefficient\n","        return pearsonr(y_pred.cpu().squeeze(), y.cpu().squeeze())[0]\n","    \n","    def loss(self, scores, pred_scores):\n","        # RMSE loss\n","        loss = (((pred_scores - scores)**2).mean())**0.5\n","\n","        return loss\n","\n","def ConvNet18():\n","    return ConvNet()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KueCXPXILP6g","colab_type":"text"},"source":["### Train Model"]},{"cell_type":"code","metadata":{"id":"hLvimzOBLP6j","colab_type":"code","colab":{}},"source":["# Used for tracking validation results\n","best_corr = 0.0\n","\n","def validation(model, model_name, val_batches, epoch, final = False):\n","    # Used for validation\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","    eval_loss = 0\n","    correlations = []\n","    \n","    # This wil keep track of best correlation achieved so far\n","    global best_corr\n","\n","    for step, batch in enumerate(val_batches):        \n","\n","        X = batch[0].to('cuda')\n","        y = batch[1].to('cuda')\n","\n","        with torch.no_grad():        \n","            y_pred = model(X)                      \n","                \n","        # Compute and record batch Pearson Correlation\n","        corr = model.check_r(y_pred, y)\n","        correlations.append(corr)\n","    if not final:    \n","    # Save model if good correlation on validation\n","        if np.mean(correlations) > best_corr and np.mean(correlations) > 0.1:\n","\n","            best_corr = np.mean(correlations)\n","            print(\"===================\")\n","            print(\"Saving a good model\")\n","            print(\"===================\")\n","            torch.save(model.state_dict(), \"./{}.pt\".format(model_name)) \n","\n","    # Save any decent enough model\n","        if np.mean(correlations) > 0.1:\n","            print(\"=====================\")\n","            print(\"Saving a decent model\")\n","            print(\"=====================\")\n","            torch.save(model.state_dict(), \"./{}_{}.pt\".format(model_name, epoch)) \n","        \n","    # Report the final accuracy for this validation run.\n","    print(f\"|  Correlation: {np.mean(correlations):.2f}     |\")\n","    print(f\"|  Validation took: {time.time() - t0:0f} s |\")\n","\n","\n","\n","def train(model, model_name, train_batches, validation_batches, test_batches, epochs):\n","    # Trains the model\n","\n","    # Keep track of losses\n","    losses = []\n","    \n","    for epoch_i in range(epochs):\n","        \n","        print(\"\")\n","        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","        print('Training...')\n","\n","        t0 = time.time()\n","        total_loss = 0\n","\n","        for step, batch in enumerate(train_batches):\n","            \n","            X = batch[0].to('cuda')\n","            y = batch[1].to('cuda')\n","    \n","            model.zero_grad()                   # Reset grads\n","            y_pred = model(X)                   # Forward pass\n","            loss = model.loss(y, y_pred)        # Compute loss\n","            total_loss += loss.item()           # Accumulate loss\n","            loss.backward()                     # Backward pass\n","\n","            optimizer.step()                    # Update params\n","\n","            if step % 140 == 0:\n","                # Progress update every 140 batches                \n","                print('  Batch {:>5,}  of  {:>5,}    |    Elapsed: {:.0f}s.'.format(step, len(train_batches), time.time() - t0))\n","                print(f'  Loss = {loss.item():.2f}')\n","\n","        # Compute and store avg loss\n","        avg_train_loss = total_loss / len(X)\n","        losses.append(avg_train_loss)\n","\n","        print(\"\")\n","        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","        print(\"  Training epoch took: {:.0f}s\".format(time.time() - t0))\n","\n","        # Validate after every epoch\n","        validation(model, model_name, validation_batches, epoch_i) \n","\n","    print(\"\")\n","    print(\"Training complete!\")\n","    print(\"\")\n","    # Validate on model selected by best Pearon Correlation\n","    print(\"======== Running final validation ========\")\n","    print(\"\")\n","    print(\"Loading best model\")\n","    model.load_state_dict(torch.load(\"./{}.pt\".format(model_name)))\n","    model.eval()\n","    \n","    # Final validation, run on hold-out set. `Final` flag does not prompt to save the model\n","    validation(model, model_name, test_batches, epoch_i, final=True) \n","\n","\n","def get_test_preds(model, batches):\n","    # Used to predict for testset\n","    torch.cuda.empty_cache()\n","    y_preds = []\n","    for batch in batches:\n","        with torch.no_grad():\n","\n","            X = batch.to(device='cuda')\n","\n","            y_pred = model(X)          \n","\n","        y_preds.append(y_pred.item())\n","    return y_preds\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dc4nt-leLP6k","colab_type":"code","colab":{}},"source":["# Both networks won't fit to GPU\n","\n","# resnet = ResNet18().to('cuda')\n","\n","cnn = ConvNet().to('cuda')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dLJ1MzulLP6m","colab_type":"code","colab":{}},"source":["optimizer = optim.AdamW(cnn.parameters(), lr=LR)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ddQa_TKzLP6o","colab_type":"code","outputId":"f8c9602f-f9f4-43e8-e3a5-2f4114c405cf","executionInfo":{"status":"ok","timestamp":1582670249278,"user_tz":0,"elapsed":303563,"user":{"displayName":"Michael Zotov","photoUrl":"","userId":"18245138602260748773"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["train(cnn, 'cnn', train_batches, validation_batches, test_batches, EPOCHS)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch     0  of    280    |    Elapsed: 0s.\n","  Loss = 1.62\n","  Batch   140  of    280    |    Elapsed: 36s.\n","  Loss = 0.64\n","\n","  Average training loss: 8.55\n","  Training epoch took: 72s\n","\n","Running Validation...\n","|  Correlation: 0.05     |\n","|  Validation took: 3.284114 s |\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch     0  of    280    |    Elapsed: 0s.\n","  Loss = 1.45\n","  Batch   140  of    280    |    Elapsed: 36s.\n","  Loss = 0.64\n","\n","  Average training loss: 8.44\n","  Training epoch took: 71s\n","\n","Running Validation...\n","|  Correlation: 0.09     |\n","|  Validation took: 3.274883 s |\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch     0  of    280    |    Elapsed: 0s.\n","  Loss = 1.45\n","  Batch   140  of    280    |    Elapsed: 36s.\n","  Loss = 0.64\n","\n","  Average training loss: 8.38\n","  Training epoch took: 72s\n","\n","Running Validation...\n","===================\n","Saving a good model\n","===================\n","=====================\n","Saving a decent model\n","=====================\n","|  Correlation: 0.12     |\n","|  Validation took: 3.288841 s |\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","  Batch     0  of    280    |    Elapsed: 0s.\n","  Loss = 1.44\n","  Batch   140  of    280    |    Elapsed: 36s.\n","  Loss = 0.64\n","\n","  Average training loss: 8.30\n","  Training epoch took: 71s\n","\n","Running Validation...\n","===================\n","Saving a good model\n","===================\n","=====================\n","Saving a decent model\n","=====================\n","|  Correlation: 0.15     |\n","|  Validation took: 3.277878 s |\n","\n","Training complete!\n","\n","======== Running final validation ========\n","\n","Loading best model\n","\n","Running Validation...\n","|  Correlation: 0.09     |\n","|  Validation took: 3.166540 s |\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X-ym66AELP6p","colab_type":"code","colab":{}},"source":["# Clear GPU cache\n","torch.cuda.empty_cache()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"keCfPdvrgUDi","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}