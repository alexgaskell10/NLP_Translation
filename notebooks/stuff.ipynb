{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cw_v1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNHhrrSWch77YHd5f8xLwPQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexgaskell10/NLP_Translation/blob/master/notebooks/stuff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpAd9yKPDnlI",
        "colab_type": "code",
        "outputId": "460d6474-090e-440e-a7c9-cc8e7f2fcf3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        }
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.4.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.10)\n",
            "Requirement already satisfied: tokenizers==0.0.11 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.11)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.2)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.10 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.10)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.10->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.10->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yKxJ_GREs_C",
        "colab_type": "code",
        "outputId": "030e121e-ee3a-431f-e52d-9cd307f2bd13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        }
      },
      "source": [
        "import torch\n",
        "from transformers import BertModel, BertTokenizer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2vlhy0vI5Zf",
        "colab_type": "text"
      },
      "source": [
        "**Loading BERT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1R2NLrgEtCf",
        "colab_type": "code",
        "outputId": "71da37c4-9287-49f9-8b4e-648cdfadb189",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "input_ids = torch.tensor([tokenizer.encode(\"Here is some text to encode\", add_special_tokens=True)])  # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n",
        "with torch.no_grad():\n",
        "    hidden_states = model(input_ids)  # Models outputs are now tuples\n",
        "    last_hidden_states = model(input_ids)[0]  # Models outputs are now tuples\n",
        "\n",
        "print(len(hidden_states), last_hidden_states.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 torch.Size([1, 9, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scs7ICZrPFcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download and unzip the data\n",
        "from os.path import exists\n",
        "if not exists('ende_data.zip'):\n",
        "    !wget -O ende_data.zip https://competitions.codalab.org/my/datasets/download/c748d2c0-d6be-4e36-9f12-ca0e88819c4d\n",
        "    !unzip ende_data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPy_iwHnOSAZ",
        "colab_type": "code",
        "outputId": "2d9fc462-795c-4939-f4fd-32737b93e137",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "# Check the files\n",
        "import io\n",
        "\n",
        "#English-German\n",
        "print(\"---EN-DE---\")\n",
        "print()\n",
        "\n",
        "with open(\"./train.ende.src\", \"r\") as ende_src:\n",
        "    print(\"Source: \",ende_src.readline())\n",
        "with open(\"./train.ende.mt\", \"r\") as ende_mt:\n",
        "    print(\"Translation: \",ende_mt.readline())\n",
        "with open(\"./train.ende.scores\", \"r\") as ende_scores:\n",
        "    print(\"Score: \",ende_scores.readline())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---EN-DE---\n",
            "\n",
            "Source:  José Ortega y Gasset visited Husserl at Freiburg in 1934.\n",
            "\n",
            "Translation:  1934 besuchte José Ortega y Gasset Husserl in Freiburg.\n",
            "\n",
            "Score:  1.1016968715664406\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yotBQbegEtVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load data into variables\n",
        "with open(\"./train.ende.src\", \"r\") as ende_src:\n",
        "    en_train = ende_src.read().split('\\n')\n",
        "with open(\"./train.ende.mt\", \"r\") as ende_src:\n",
        "    de_train = ende_src.read().split('\\n')\n",
        "with open(\"./train.ende.scores\", \"r\") as ende_src:\n",
        "    train_scores = ende_src.read().split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB6TjMiggdX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert input sequences to correct format\n",
        "\n",
        "# Tokenize English\n",
        "inputs_en = []\n",
        "max_len_en = 0\n",
        "    \n",
        "for i in range(len(en_train)-1):\n",
        "    seq = en_train[i][:-1]\n",
        "    input_ids = torch.tensor([tokenizer.encode(seq, add_special_tokens=True)])  # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n",
        "    inputs_en.append(input_ids)\n",
        "    if input_ids.shape[-1] > max_len_en:\n",
        "        max_len_en = input_ids.shape[-1]\n",
        "\n",
        "# Tokenize German\n",
        "inputs_de = []\n",
        "max_len_de = 0\n",
        "\n",
        "for i in range(len(en_train)-1):\n",
        "    seq = de_train[i][:-1]\n",
        "    input_ids = torch.tensor([tokenizer.encode(seq, add_special_tokens=True)])  # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n",
        "    inputs_de.append(input_ids)\n",
        "    if input_ids.shape[-1] > max_len_de:\n",
        "        max_len_de = input_ids.shape[-1]\n",
        "\n",
        "# Combine tokens into single\n",
        "inp_tensor = torch.zeros((len(inputs_en), max_len_en + max_len_de - 2))      # <-- -2 because special tokens are not necessary at beginning of German sequence\n",
        "\n",
        "for i in range(len(inputs_en)):\n",
        "    # Add English tokens\n",
        "    en_tokens = inputs_en[i].squeeze()\n",
        "    inp_tensor[i, : len(en_tokens)] = en_tokens\n",
        "\n",
        "    # Add German tokens\n",
        "    de_tokens = inputs_de[i][:,2:].squeeze()      # <-- ignore first 2 tokens as these are special tokens and unnecessary in this case\n",
        "    inp_tensor[i, max_len_en : max_len_en + len(de_tokens)] = de_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "19ae9aaf-4ca0-4f9e-81aa-8de286c1e527",
        "id": "Rp7NCpb-9NL-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "USE_GPU = True\n",
        "dtype = torch.float32 \n",
        "\n",
        "if USE_GPU and torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "batches = torch.split(inp_tensor)\n",
        "X = inp_tensor[:1000].to(device=device, dtype=torch.long)\n",
        "model = model.to(device=device)\n",
        "with torch.no_grad():\n",
        "    last_hidden_states = model(X)[0]  # Models outputs are now tuples\n",
        "\n",
        "print(last_hidden_states.shape)\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1000, 132, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}