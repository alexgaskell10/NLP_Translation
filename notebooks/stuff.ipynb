{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"stuff.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github","colab_type":"text"},"source":["<a href=\"https://colab.research.google.com/github/alexgaskell10/NLP_Translation/blob/master/notebooks/stuff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","metadata":{"id":"hpAd9yKPDnlI","colab_type":"code","outputId":"6c61fcfa-5fc4-4764-8f03-ae9b3db7c35a","colab":{"base_uri":"https://localhost:8080/","height":378},"executionInfo":{"status":"ok","timestamp":1581334801602,"user_tz":0,"elapsed":4838,"user":{"displayName":"Alex Gaskell","photoUrl":"","userId":"12223185016372309013"}}},"source":["! pip install transformers"],"execution_count":63,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.4.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.10)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tokenizers==0.0.11 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.11)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n","Requirement already satisfied: botocore<1.15.0,>=1.14.10 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.10)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.10->boto3->transformers) (2.6.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.10->boto3->transformers) (0.15.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8yKxJ_GREs_C","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":62},"outputId":"ff8d3155-c4ac-4fb9-b78b-ac58bdfc09dc","executionInfo":{"status":"ok","timestamp":1581337656521,"user_tz":0,"elapsed":2221,"user":{"displayName":"Alex Gaskell","photoUrl":"","userId":"12223185016372309013"}}},"source":["import torch\n","from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n","from sklearn.model_selection import train_test_split\n","from torch.nn import Linear\n","from torch import optim\n","import random\n","import time\n","import numpy as np\n","from scipy.stats import pearsonr\n","torch.set_printoptions(4)"],"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"scs7ICZrPFcs","colab_type":"code","colab":{}},"source":["# Download and unzip the data\n","from os.path import exists\n","if not exists('ende_data.zip'):\n","    !wget -O ende_data.zip https://competitions.codalab.org/my/datasets/download/c748d2c0-d6be-4e36-9f12-ca0e88819c4d\n","    !unzip ende_data.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yotBQbegEtVA","colab_type":"code","colab":{}},"source":["# Load data into variables\n","with open(\"./train.ende.src\", \"r\") as ende_src:\n","    en_corpus = ende_src.read().split('\\n')[:-1]\n","with open(\"./train.ende.mt\", \"r\") as ende_src:\n","    de_corpus = ende_src.read().split('\\n')[:-1]\n","with open(\"./train.ende.scores\", \"r\") as ende_src:\n","    scores = [float(x) for x in ende_src.read().split('\\n')[:-1]]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rvu5GAlL5-wc","colab_type":"code","colab":{}},"source":["class Preprocessor:\n","    def __init__(self, device, bert_config):\n","        self.device = device\n","        self.tokenizer = BertTokenizer.from_pretrained(bert_config)\n","\n","    def get_input_tensor(self, en_corpus, de_corpus):\n","        # Convert input sequences to correct format\n","        N = len(en_corpus)\n","\n","        # Tokenize corpora\n","        inputs_en, inputs_de = [],[]\n","        max_len_en, max_len_de = 0,0\n","            \n","        for i in range(N):\n","            # English\n","            seq = en_corpus[i][:-1]\n","            input_ids = torch.tensor([self.tokenizer.encode(seq, add_special_tokens=True)])\n","            inputs_en.append(input_ids)\n","            if input_ids.shape[-1] > max_len_en:\n","                max_len_en = input_ids.shape[-1]\n","\n","            # German\n","            seq = de_corpus[i][:-1]\n","            input_ids = torch.tensor([self.tokenizer.encode(seq, add_special_tokens=True)])\n","            inputs_de.append(input_ids)\n","            if input_ids.shape[-1] > max_len_de:\n","                max_len_de = input_ids.shape[-1]\n","\n","        # Combine tokens into single tensor\n","        inp_tensor = torch.zeros((N, max_len_en + max_len_de - 2))      # <-- -2 because special tokens are not necessary at beginning of German sequence\n","\n","        for i in range(N):\n","            # Add English tokens\n","            en_tokens = inputs_en[i].squeeze()\n","            inp_tensor[i, : len(en_tokens)] = en_tokens\n","\n","            # Add German tokens\n","            de_tokens = inputs_de[i][:,2:].squeeze()      # <-- ignore first 2 tokens as these are special tokens and unnecessary in this case\n","            inp_tensor[i, max_len_en : max_len_en + len(de_tokens)] = de_tokens\n","\n","        return inp_tensor\n","\n","    def attention_mask(self, list_input_tensor):\n","        list_attention_masks = []\n","        for input_tensor in list_input_tensor:\n","            attention_mask = torch.zeros((input_tensor.shape))\n","            attention_mask[input_tensor != 0] = 1\n","            list_attention_masks.append(attention_mask)\n","\n","        return list_attention_masks\n","\n","    def get_batches(self, inp_tensor, scores, BATCH_N):\n","        inp_tensor = inp_tensor\n","        scores = torch.tensor(scores).view(-1,1)\n","\n","        # Split data into train, test and val batches\n","        inp_tensor_batches = torch.split(inp_tensor, BATCH_N)\n","        scores_batches = torch.split(scores, BATCH_N)\n","        X_train_val, X_test, y_train_val, y_test = train_test_split(inp_tensor_batches, scores_batches, random_state=1, test_size=0.1)\n","        X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=1, test_size=1/9)\n","        print(len(X_train), X_train[0].shape)\n","\n","        # Create attention masks\n","        X_train_mask = self.attention_mask(X_train)\n","        X_val_mask = self.attention_mask(X_val) \n","        X_test_mask = self.attention_mask(X_test)\n","        print(len(X_train_mask), X_train_mask[0].shape)\n","\n","        # Batch X, mask and y together\n","        batches_train = [(X, mask, y) for X,mask,y in zip(X_train, X_train_mask, y_train)]\n","        batches_val = [(X, mask, y) for X,mask,y in zip(X_val, X_val_mask, y_val)]\n","        batches_test = [(X, mask, y) for X,mask,y in zip(X_test, X_test_mask, y_test)]\n","\n","        return batches_train, batches_val, batches_test\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LWG59VN-23aV","colab_type":"code","colab":{}},"source":["class BertRegressor:\n","    def __init__(self, input_shape, bert_config, device):\n","        self.device = device\n","        self.bert = BertModel.from_pretrained(bert_config).to(device)\n","        self.linear = Linear(in_features = input_shape[-1] * 768,                   # <-- 768 is the dim of Bert embedding\n","                             out_features = 1, bias = True).to(device)\n","\n","    def forward(self, X, attention_mask=None):\n","        if attention_mask is not None:\n","            X = self.bert(X, attention_mask)[0].view(X.shape[0], -1)    \n","        else:\n","            X = self.bert(X)[0].view(X.shape[0], -1)\n","\n","        preds = self.linear(X)\n","        return preds\n","\n","    def loss(self, scores, pred_scores):\n","        rmse = (((pred_scores - scores)**2).mean())**0.5\n","        return rmse\n","\n","    def check_r(self, y_pred, y):\n","        return pearsonr(y_pred.cpu().squeeze(), y.cpu().squeeze())[0]\n","\n","    def zero_grad(self):\n","        self.bert.zero_grad()\n","        self.linear.zero_grad()\n","\n","    def params(self):\n","        return list(self.bert.parameters()) + list(self.linear.parameters())\n","\n","    def __call__(self, X, attention_mask=None):\n","        return self.forward(X, attention_mask)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Rp7NCpb-9NL-","colab":{"base_uri":"https://localhost:8080/","height":49},"outputId":"91d13400-dff2-43c5-b7bc-c48770552b45","executionInfo":{"status":"ok","timestamp":1581338082306,"user_tz":0,"elapsed":5484,"user":{"displayName":"Alex Gaskell","photoUrl":"","userId":"12223185016372309013"}}},"source":["USE_GPU = True\n","\n","if USE_GPU and torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","\n","bert_config = 'bert-base-multilingual-cased'\n","BATCH_N = 32\n","EPOCHS = 4\n","LR = 2e-5\n","\n","# Set seeds\n","seed_val = 111\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# Preprocessing\n","preprocessor = Preprocessor(device, bert_config)\n","input_tensor = preprocessor.get_input_tensor(en_corpus, de_corpus)\n","batches_train, batches_val, batches_test = preprocessor.get_batches(input_tensor, scores, BATCH_N)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["175 torch.Size([32, 132])\n","175 torch.Size([32, 132])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4m8jwjBaCsAJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":32},"outputId":"5fff81b9-a589-48cb-e14d-c17e84306aff","executionInfo":{"status":"ok","timestamp":1581338132989,"user_tz":0,"elapsed":4050,"user":{"displayName":"Alex Gaskell","photoUrl":"","userId":"12223185016372309013"}}},"source":["# torch.cuda.empty_cache()\n","# model = BertRegressor(batches_train[0][0].shape, bert_config, device)\n","\n","# X = batches_train[0][0].to(device, dtype=torch.long)\n","# mask = batches_train[0][1].to(device)\n","# # print(X.device, mask.device, X.requires_grad)\n","# model(X, mask).shape"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([32, 1])"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"8306qtg603CS","colab_type":"code","colab":{}},"source":["def validation(val_batches):\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","    eval_loss = 0\n","    correlations = []\n","\n","    for batch in val_batches:        \n","\n","        # Untie batch and put on GPU\n","        X = batch[0].to(device=device, dtype=torch.long)\n","        mask = batch[1].to(device)\n","        y = batch[2].to(device)\n","\n","        with torch.no_grad():        \n","            y_pred = model(X)                      \n","                \n","        # Compute and record batch accuracy\n","        corr = model.check_r(y_pred, y)\n","        correlations.append(corr)\n","\n","    # Report the final accuracy for this validation run.\n","    print(f\"|  Correlation: {np.mean(correlations):.2f}     |\")\n","    print(f\"|  Validation took: {time.time() - t0:0f} s |\")\n","\n","    torch.cuda.empty_cache()\n","\n","\n","def train(model, train_batches, val_batches, epochs):\n","\n","    losses = []\n","    \n","    for epoch_i in range(epochs):\n","        \n","        print(\"\")\n","        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","        print('Training...')\n","\n","        t0 = time.time()\n","        total_loss = 0\n","\n","        for step, batch in enumerate(train_batches):\n","\n","            if step % 20 == 0 and not step == 0:\n","                # Progress update every 40 batches                \n","                print('  Batch {:>5,}  of  {:>5,}    |    Elapsed: {:.0f}s.'.format(step, len(train_batches), time.time() - t0))\n","\n","            # Untie batch and put on GPU\n","            X = batch[0].to(device=device, dtype=torch.long)\n","            mask = batch[1].to(device)\n","            y = batch[2].to(device)\n","\n","            model.zero_grad()                   # Reset grads\n","            y_pred = model(X, mask)             # Forward pass\n","            loss = model.loss(y, y_pred)        # Compute loss\n","            total_loss += loss.item()           # Accumulate loss\n","            loss.backward()                     # Backward pass\n","\n","            optimizer.step()                    # Update params\n","            scheduler.step()                    # Update optimizer\n","\n","            torch.cuda.empty_cache()            # Clear GPU cache to avoid memory issues\n","\n","        # Compute and store avg loss\n","        avg_train_loss = total_loss / len(X)\n","        losses.append(avg_train_loss)\n","\n","        print(\"\")\n","        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","        print(\"  Training epoch took: {:.0f}s\".format(time.time() - t0))\n","\n","        validation(val_batches) \n","\n","    print(\"\")\n","    print(\"Training complete!\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ixlL-kPPtjZv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":776},"outputId":"9afcd6e3-01d2-444c-f684-b4cc57fba48c","executionInfo":{"status":"error","timestamp":1581339011592,"user_tz":0,"elapsed":140723,"user":{"displayName":"Alex Gaskell","photoUrl":"","userId":"12223185016372309013"}}},"source":["# Model to train\n","model = BertRegressor(batches_train[0][0].shape, bert_config, device)\n","\n","# Optimizer and scheduler\n","optimizer = AdamW(model.params(), lr=LR, eps=1e-8)\n","total_steps = len(batches_train[0]) * EPOCHS // BATCH_N\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0,\n","                                            num_training_steps = total_steps)\n","\n","torch.cuda.empty_cache()\n","train(model, batches_train[:60], batches_val[:60], epochs=EPOCHS)"],"execution_count":35,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch    20  of     60    |    Elapsed: 18s.\n","  Batch    40  of     60    |    Elapsed: 36s.\n","\n","  Average training loss: 1.57\n","  Training epcoh took: 55s\n","\n","Running Validation...\n","|  Correlation: -0.05     |\n","|  Validation took: 6.016750 s |\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch    20  of     60    |    Elapsed: 18s.\n","  Batch    40  of     60    |    Elapsed: 36s.\n","\n","  Average training loss: 1.57\n","  Training epcoh took: 54s\n","\n","Running Validation...\n","|  Correlation: -0.05     |\n","|  Validation took: 6.076826 s |\n","\n","======== Epoch 3 / 4 ========\n","Training...\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-f9c77e113258>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-33-17f44415bedd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_batches, val_batches, epochs)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m# Accumulate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                     \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                    \u001b[0;31m# Update params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"__5QoNXSsz38","colab_type":"code","colab":{}},"source":["# Get all of the model's parameters as a list of tuples.\n","params = list(model.named_parameters())\n","\n","print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n","\n","print('==== Embedding Layer ====\\n')\n","\n","for p in params[0:5]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== First Transformer ====\\n')\n","\n","for p in params[5:21]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== Output Layer ====\\n')\n","\n","for p in params[-4:]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","    "],"execution_count":0,"outputs":[]}]}