{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cw_v1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN9kAM/ACOlkdiiAIYkj6oO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexgaskell10/NLP_Translation/blob/master/notebooks/stuff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpAd9yKPDnlI",
        "colab_type": "code",
        "outputId": "cb046d03-de56-48dd-edde-db38f94b5d94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        }
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/fc/bd726a15ab2c66dc09306689d04da07a3770dad724f0883f0a4bfb745087/transformers-2.4.1-py3-none-any.whl (475kB)\n",
            "\r\u001b[K     |▊                               | 10kB 30.5MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20kB 6.0MB/s eta 0:00:01\r\u001b[K     |██                              | 30kB 8.5MB/s eta 0:00:01\r\u001b[K     |██▊                             | 40kB 5.6MB/s eta 0:00:01\r\u001b[K     |███▍                            | 51kB 6.8MB/s eta 0:00:01\r\u001b[K     |████▏                           | 61kB 8.1MB/s eta 0:00:01\r\u001b[K     |████▉                           | 71kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 81kB 10.3MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 92kB 11.3MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 102kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 112kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 122kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 133kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 143kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 153kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 163kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 174kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 184kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 194kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 204kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 215kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 225kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 235kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 245kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 256kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 266kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 276kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 286kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 296kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 307kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 317kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 327kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 337kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 348kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 358kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 368kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 378kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 389kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 399kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 409kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 419kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 430kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 440kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 450kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 460kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 471kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 481kB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers==0.0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/36/7af38d572c935f8e0462ec7b4f7a46d73a2b3b1a938f50a5e8132d5b2dc5/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 57.1MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 55.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 36.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.2)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.10 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.10->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.10->boto3->transformers) (2.6.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=05d75584da6807149d20921041583c9964f15250507e36e5b602544bf8ef3b71\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.0.11 transformers-2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yKxJ_GREs_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from transformers import BertModel, BertTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2vlhy0vI5Zf",
        "colab_type": "text"
      },
      "source": [
        "**Loading BERT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1R2NLrgEtCf",
        "colab_type": "code",
        "outputId": "0ce1dbcd-02c0-4fa0-98b6-81ed164d3958",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "input_ids = torch.tensor([tokenizer.encode(\"Here is some text to encode\", add_special_tokens=True)])  # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n",
        "with torch.no_grad():\n",
        "    hidden_states = model(input_ids)  # Models outputs are now tuples\n",
        "    last_hidden_states = model(input_ids)[0]  # Models outputs are now tuples\n",
        "\n",
        "print(len(hidden_states), last_hidden_states.shape)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 torch.Size([1, 9, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5WJObfbEtFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scs7ICZrPFcs",
        "colab_type": "code",
        "outputId": "9e0a8089-8a02-450a-9ea1-c7001ed6478e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        }
      },
      "source": [
        "# Download and unzip the data\n",
        "from os.path import exists\n",
        "if not exists('ende_data.zip'):\n",
        "    !wget -O ende_data.zip https://competitions.codalab.org/my/datasets/download/c748d2c0-d6be-4e36-9f12-ca0e88819c4d\n",
        "    !unzip ende_data.zip"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-09 15:20:14--  https://competitions.codalab.org/my/datasets/download/c748d2c0-d6be-4e36-9f12-ca0e88819c4d\n",
            "Resolving competitions.codalab.org (competitions.codalab.org)... 129.175.22.230\n",
            "Connecting to competitions.codalab.org (competitions.codalab.org)|129.175.22.230|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://newcodalab.lri.fr/prod-private/dataset_data_file/None/104ea/en-de.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=81c248a4d265afaa9aaa003646668a02145450ccd89354f384c40f1beb542b7b&X-Amz-Date=20200209T152014Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200209%2Fnewcodalab%2Fs3%2Faws4_request [following]\n",
            "--2020-02-09 15:20:14--  https://newcodalab.lri.fr/prod-private/dataset_data_file/None/104ea/en-de.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=81c248a4d265afaa9aaa003646668a02145450ccd89354f384c40f1beb542b7b&X-Amz-Date=20200209T152014Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200209%2Fnewcodalab%2Fs3%2Faws4_request\n",
            "Resolving newcodalab.lri.fr (newcodalab.lri.fr)... 129.175.15.11\n",
            "Connecting to newcodalab.lri.fr (newcodalab.lri.fr)|129.175.15.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 864010 (844K) [application/zip]\n",
            "Saving to: ‘ende_data.zip’\n",
            "\n",
            "\rende_data.zip         0%[                    ]       0  --.-KB/s               \rende_data.zip       100%[===================>] 843.76K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2020-02-09 15:20:15 (10.2 MB/s) - ‘ende_data.zip’ saved [864010/864010]\n",
            "\n",
            "Archive:  ende_data.zip\n",
            "  inflating: dev.ende.mt             \n",
            "  inflating: dev.ende.scores         \n",
            "  inflating: dev.ende.src            \n",
            "  inflating: test.ende.mt            \n",
            "  inflating: test.ende.src           \n",
            "  inflating: train.ende.mt           \n",
            "  inflating: train.ende.scores       \n",
            "  inflating: train.ende.src          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPy_iwHnOSAZ",
        "colab_type": "code",
        "outputId": "ec0606b8-45c3-4526-eb4e-e3369216faa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "# Check the files\n",
        "import io\n",
        "\n",
        "#English-German\n",
        "print(\"---EN-DE---\")\n",
        "print()\n",
        "\n",
        "with open(\"./train.ende.src\", \"r\") as ende_src:\n",
        "    print(\"Source: \",ende_src.readline())\n",
        "with open(\"./train.ende.mt\", \"r\") as ende_mt:\n",
        "    print(\"Translation: \",ende_mt.readline())\n",
        "with open(\"./train.ende.scores\", \"r\") as ende_scores:\n",
        "    print(\"Score: \",ende_scores.readline())\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---EN-DE---\n",
            "\n",
            "Source:  José Ortega y Gasset visited Husserl at Freiburg in 1934.\n",
            "\n",
            "Translation:  1934 besuchte José Ortega y Gasset Husserl in Freiburg.\n",
            "\n",
            "Score:  1.1016968715664406\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yotBQbegEtVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load data into variables\n",
        "with open(\"./train.ende.src\", \"r\") as ende_src:\n",
        "    en_train = ende_src.read().split('\\n')\n",
        "with open(\"./train.ende.mt\", \"r\") as ende_src:\n",
        "    de_train = ende_src.read().split('\\n')\n",
        "with open(\"./train.ende.scores\", \"r\") as ende_src:\n",
        "    train_scores = ende_src.read().split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB6TjMiggdX1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "e91b13d0-e4ba-49c2-8d10-00f9aa515822"
      },
      "source": [
        "# Convert input sequences to correct format\n",
        "\n",
        "# Tokenize English\n",
        "inputs_en = []\n",
        "max_len_en = 0\n",
        "    \n",
        "for i in range(len(en_train)):\n",
        "    seq = en_train[i][:-1]\n",
        "    input_ids = torch.tensor([tokenizer.encode(seq, add_special_tokens=True)])  # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n",
        "    inputs_en.append(input_ids)\n",
        "    if input_ids.shape[-1] > max_len_en:\n",
        "        max_len_en = input_ids.shape[-1]\n",
        "\n",
        "# Tokenize German\n",
        "inputs_de = []\n",
        "max_len_de = 0\n",
        "\n",
        "for i in range(len(de_train)):\n",
        "    seq = de_train[i][:-1]\n",
        "    input_ids = torch.tensor([tokenizer.encode(seq, add_special_tokens=True)])  # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n",
        "    inputs_de.append(input_ids)\n",
        "    if input_ids.shape[-1] > max_len_de:\n",
        "        max_len_de = input_ids.shape[-1]"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa0GeiFuxObI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "54a6d978-c308-489f-828f-db902c745077"
      },
      "source": [
        "print(max_len_en, max_len_de)\n",
        "\n",
        "inp_tensor = torch.zeros((len(en_train), max_len_en + max_len_de + 1))      # + 1 to allow [SEP] at end of sequence\n",
        "\n",
        "for i in range(len(en_train[:3])):\n",
        "    # Add English tokens\n",
        "    en_tokens = inputs_en[i].squeeze()\n",
        "    inp_tensor[i, : len(en_tokens)] = en_tokens\n",
        "\n",
        "    # Add German tokens\n",
        "    de_tokens = inputs_de[i][2:].squeeze()\n",
        "    inp_tensor[i, max_len_en : max_len_en + len(de_tokens)] = de_tokens\n",
        "\n",
        "#     # Add [SEP] token to end of sentence\n",
        "\n",
        "\n",
        "# print(inp_tensor[:3])"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "66 70\n",
            "[101, 101, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jo9JQjXHnFdw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "e87e850b-e76a-4179-f978-7468bd3ead65"
      },
      "source": [
        "# Tokenize input sequence\n",
        "# print(max_len)\n",
        "# print(inputs[0].shape, inputs[1].shape)\n",
        "# for inp in inputs[:10]:\n",
        "#     with torch.no_grad():\n",
        "#         hidden_states = model(input_ids)  # Models outputs are now tuples\n",
        "#         last_hidden_states = model(input_ids)[0]  # Models outputs are now tuples\n",
        "\n",
        "a = torch.randint(3,5,(2,50))\n",
        "# a = inputs[0] \n",
        "with torch.no_grad():\n",
        "    hidden_states = model(a)  # Models outputs are now tuples\n",
        "    last_hidden_states = model(a)[0]  # Models outputs are now tuples\n",
        "print(hidden_states[0].shape)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 50, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}