{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert_multilingual.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github","colab_type":"text"},"source":["<a href=\"https://colab.research.google.com/github/alexgaskell10/NLP_Translation/blob/master/notebooks/stuff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","metadata":{"id":"hpAd9yKPDnlI","colab_type":"code","outputId":"f7a9b232-ee6a-47bc-c0a9-75723accd322","executionInfo":{"status":"ok","timestamp":1581878371743,"user_tz":0,"elapsed":4471,"user":{"displayName":"Alex Gaskell","photoUrl":"","userId":"12223185016372309013"}},"colab":{"base_uri":"https://localhost:8080/","height":378}},"source":["! pip install transformers"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.4.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n","Requirement already satisfied: tokenizers==0.0.11 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n","Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8yKxJ_GREs_C","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":62},"outputId":"d6bbec08-6cc0-45a6-cf7c-59eac8b6b36b","executionInfo":{"status":"ok","timestamp":1581878373759,"user_tz":0,"elapsed":6470,"user":{"displayName":"Alex Gaskell","photoUrl":"","userId":"12223185016372309013"}}},"source":["import torch\n","from torch.nn import Linear, ModuleList\n","from torch import optim\n","import torch.nn.functional as F\n","from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n","from sklearn.model_selection import train_test_split\n","import random\n","import time\n","import numpy as np\n","from scipy.stats import pearsonr\n","import matplotlib.pyplot as plt\n","torch.set_printoptions(2)"],"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"scs7ICZrPFcs","colab_type":"code","outputId":"6954658b-fd26-43a9-a67e-30a56ed8394b","executionInfo":{"status":"ok","timestamp":1581878373760,"user_tz":0,"elapsed":6460,"user":{"displayName":"Alex Gaskell","photoUrl":"","userId":"12223185016372309013"}},"colab":{"base_uri":"https://localhost:8080/","height":65}},"source":["# ############ ENGLISH - CHINESE ############\n","\n","# Download and unzip the data\n","from os.path import exists\n","if not exists('enzh_data.zip'):\n","    !wget -O enzh_data.zip https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n","    !unzip enzh_data.zip\n","\n","# Load data into variables\n","with open(\"./train.enzh.src\", \"r\") as ende_src:\n","    en_corpus = ende_src.read().split('\\n')[:-1]\n","with open(\"./train.enzh.mt\", \"r\") as ende_src:\n","    de_corpus = ende_src.read().split('\\n')[:-1]\n","with open(\"./train.enzh.scores\", \"r\") as ende_src:\n","    scores = [float(x) for x in ende_src.read().split('\\n')[:-1]]\n","\n","print(en_corpus[0])\n","print(de_corpus[0])\n","print(scores[0])"],"execution_count":6,"outputs":[{"output_type":"stream","text":["The last conquistador then rides on with his sword drawn.\n","最后的征服者骑着他的剑继续前进.\n","-1.5284005772625449\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yotBQbegEtVA","colab_type":"code","colab":{}},"source":["# ########### ENGLISH - GERMAN ############\n","\n","# # Download and unzip the data\n","# from os.path import exists\n","# if not exists('ende_data.zip'):\n","#     !wget -O ende_data.zip https://competitions.codalab.org/my/datasets/download/c748d2c0-d6be-4e36-9f12-ca0e88819c4d\n","#     !unzip ende_data.zip\n","\n","# # Load data into variables\n","# with open(\"./train.ende.src\", \"r\") as ende_src:\n","#     en_corpus = ende_src.read().split('\\n')[:-1]\n","# with open(\"./train.ende.mt\", \"r\") as ende_src:\n","#     de_corpus = ende_src.read().split('\\n')[:-1]\n","# with open(\"./train.ende.scores\", \"r\") as ende_src:\n","#     scores = [float(x) for x in ende_src.read().split('\\n')[:-1]]\n","\n","# print(en_corpus[0])\n","# print(de_corpus[0])\n","# print(scores[0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XizhBzbFtuic","colab_type":"code","outputId":"809107d0-5059-45c0-9865-430c02d42adb","executionInfo":{"status":"ok","timestamp":1581878374290,"user_tz":0,"elapsed":6969,"user":{"displayName":"Alex Gaskell","photoUrl":"","userId":"12223185016372309013"}},"colab":{"base_uri":"https://localhost:8080/","height":284}},"source":["plt.plot(sorted(scores))\n","plt.ylim([-2,2])"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(-2, 2)"]},"metadata":{"tags":[]},"execution_count":8},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"rvu5GAlL5-wc","colab_type":"code","colab":{}},"source":["class Preprocessor:\n","    def __init__(self, device, bert_config):\n","        self.device = device\n","        self.tokenizer = BertTokenizer.from_pretrained(bert_config)\n","\n","    def get_input_tensor(self, en_corpus, de_corpus):\n","        # Convert input sequences to correct format\n","        N = len(en_corpus)\n","\n","        # Tokenize corpora\n","        inputs_en, inputs_de = [],[]\n","        max_len_en, max_len_de = 0,0\n","            \n","        for i in range(N):\n","            # English\n","            seq = en_corpus[i][:-1]\n","            input_ids = torch.tensor([self.tokenizer.encode(seq, add_special_tokens=True)])\n","            inputs_en.append(input_ids)\n","            if input_ids.shape[-1] > max_len_en:\n","                max_len_en = input_ids.shape[-1]\n","\n","            # German\n","            seq = de_corpus[i][:-1]\n","            input_ids = torch.tensor([self.tokenizer.encode(seq, add_special_tokens=True)])\n","            inputs_de.append(input_ids)\n","            if input_ids.shape[-1] > max_len_de:\n","                max_len_de = input_ids.shape[-1]\n","\n","        # Combine tokens into single tensor\n","        inp_tensor = torch.zeros((N, max_len_en + max_len_de - 2))      # <-- -2 because special tokens are not necessary at beginning of German sequence\n","\n","        for i in range(N):\n","            # Add English tokens\n","            en_tokens = inputs_en[i].squeeze()\n","            inp_tensor[i, : len(en_tokens)] = en_tokens\n","\n","            # Add German tokens\n","            de_tokens = inputs_de[i][:,2:].squeeze()      # <-- ignore first 2 tokens as these are special tokens and unnecessary in this case\n","            inp_tensor[i, max_len_en : max_len_en + len(de_tokens)] = de_tokens\n","\n","        return inp_tensor\n","\n","    def attention_mask(self, list_input_tensor):\n","        list_attention_masks = []\n","        for input_tensor in list_input_tensor:\n","            attention_mask = torch.zeros((input_tensor.shape))\n","            attention_mask[input_tensor != 0] = 1\n","            list_attention_masks.append(attention_mask)\n","\n","        return list_attention_masks\n","\n","    def get_batches(self, inp_tensor, scores, BATCH_N):\n","        inp_tensor = inp_tensor\n","        scores = torch.tensor(scores).view(-1,1)\n","\n","        # Split data into train, test and val batches\n","        inp_tensor_batches = torch.split(inp_tensor, BATCH_N)\n","        scores_batches = torch.split(scores, BATCH_N)\n","        X_train_val, X_test, y_train_val, y_test = train_test_split(inp_tensor_batches, scores_batches, random_state=1, test_size=0.1)\n","        X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=1, test_size=1/9)\n","        print(len(X_train), X_train[0].shape)\n","\n","        # Create attention masks\n","        X_train_mask = self.attention_mask(X_train)\n","        X_val_mask = self.attention_mask(X_val) \n","        X_test_mask = self.attention_mask(X_test)\n","        print(len(X_train_mask), X_train_mask[0].shape)\n","\n","        # Batch X, mask and y together\n","        batches_train = [(X, mask, y) for X,mask,y in zip(X_train, X_train_mask, y_train)]\n","        batches_val = [(X, mask, y) for X,mask,y in zip(X_val, X_val_mask, y_val)]\n","        batches_test = [(X, mask, y) for X,mask,y in zip(X_test, X_test_mask, y_test)]\n","\n","        return batches_train, batches_val, batches_test\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LWG59VN-23aV","colab_type":"code","colab":{}},"source":["class BertRegressor:\n","    def __init__(self, input_shape, bert_config, device):\n","        self.device = device\n","        self.bert = BertModel.from_pretrained(bert_config).to(device)\n","        self.layers = ModuleList([\n","            Linear(in_features=input_shape[-1], out_features=1, bias=True).to(device),\n","            # Linear(in_features=input_shape[-1]*768, out_features=100, bias=True).to(device),            # <-- 768 is the dim of Bert embedding\n","            # Linear(in_features=100, out_features=1, bias=True).to(device),\n","        ])\n","\n","    def forward_1(self, X, attention_mask=None):\n","        if attention_mask is not None:\n","            X = self.bert(X, attention_mask)[0].view(X.shape[0], -1)    \n","        else:\n","            X = self.bert(X)[0].view(X.shape[0], -1)\n","\n","        X = self.layers[0](X)\n","        for l in self.layers[1:]:\n","            X = F.relu(X)\n","            X = l(X)\n","\n","        return X\n","\n","    def forward_2(self, X, attention_mask=None):\n","        if attention_mask is not None:\n","            X = self.bert(X, attention_mask)[0][:,:,0]\n","        else:\n","            X = self.bert(X)[0][:,:,0]\n","\n","        X = self.layers[0](X)\n","        for l in self.layers[1:]:\n","            X = F.relu(X)\n","            X = l(X)\n","\n","        return X\n","\n","\n","    def loss(self, scores, pred_scores):\n","        rmse = (((pred_scores - scores)**2).mean())**0.5\n","        return rmse\n","\n","    def check_r(self, y_pred, y):\n","        return pearsonr(y_pred.cpu().squeeze(), y.cpu().squeeze())[0]\n","\n","    def zero_grad(self):\n","        self.bert.zero_grad()\n","        for l in self.layers:\n","            l.zero_grad()\n","\n","    def params(self):\n","        params = list(self.bert.parameters())\n","        for l in self.layers:\n","            params.extend(list(l.parameters()))\n","        return params\n","\n","    def __call__(self, X, attention_mask=None):\n","        return self.forward_2(X, attention_mask)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Rp7NCpb-9NL-","outputId":"6716032b-e491-4e9a-a91e-647412e28d2c","executionInfo":{"status":"ok","timestamp":1581878378624,"user_tz":0,"elapsed":11269,"user":{"displayName":"Alex Gaskell","photoUrl":"","userId":"12223185016372309013"}},"colab":{"base_uri":"https://localhost:8080/","height":49}},"source":["USE_GPU = True\n","\n","if USE_GPU and torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","\n","bert_config = 'bert-base-multilingual-cased'\n","BATCH_N = 16\n","EPOCHS = 2\n","LR = 2e-5\n","\n","# Set seeds\n","seed_val = 111\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# Preprocessing\n","preprocessor = Preprocessor(device, bert_config)\n","input_tensor = preprocessor.get_input_tensor(en_corpus, de_corpus)\n","batches_train, batches_val, batches_test = preprocessor.get_batches(input_tensor, scores, BATCH_N)\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["350 torch.Size([16, 140])\n","350 torch.Size([16, 140])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4m8jwjBaCsAJ","colab_type":"code","colab":{}},"source":["# torch.cuda.empty_cache()\n","# model = BertRegressor(batches_train[0][0].shape, bert_config, device)\n","\n","# X = batches_train[0][0].to(device, dtype=torch.long)\n","# mask = batches_train[0][1].to(device)\n","# # print(X.device, mask.device, X.requires_grad)\n","# print(model(X, mask).shape)\n","# # print(model.bert.forward(X)[0][:,:,0].shape)\n","# torch.cuda.empty_cache()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8306qtg603CS","colab_type":"code","colab":{}},"source":["def validation(val_batches):\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","    eval_loss = 0\n","    correlations = []\n","\n","    for batch in val_batches:        \n","\n","        # Untie batch and put on GPU\n","        X = batch[0].to(device=device, dtype=torch.long)\n","        mask = batch[1].to(device)\n","        y = batch[2].to(device)\n","\n","        with torch.no_grad():        \n","            y_pred = model(X)                      \n","                \n","        # Compute and record batch accuracy\n","        corr = model.check_r(y_pred, y)\n","        correlations.append(corr)\n","\n","    # Report the final accuracy for this validation run.\n","    print(f\"|  Correlation: {np.mean(correlations):.2f}     |\")\n","    print(f\"|  Validation took: {time.time() - t0:0f} s |\")\n","\n","    torch.cuda.empty_cache()\n","\n","\n","def train(model, train_batches, val_batches, epochs):\n","\n","    losses = []\n","    \n","    for epoch_i in range(epochs):\n","        \n","        print(\"\")\n","        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","        print('Training...')\n","\n","        t0 = time.time()\n","        total_loss = 0\n","\n","        for step, batch in enumerate(train_batches):\n","\n","            # Untie batch and put on GPU\n","            X = batch[0].to(device=device, dtype=torch.long)\n","            mask = batch[1].to(device)\n","            y = batch[2].to(device)\n","\n","            model.zero_grad()                   # Reset grads\n","            y_pred = model(X, mask)             # Forward pass\n","            loss = model.loss(y, y_pred)        # Compute loss\n","            total_loss += loss.item()           # Accumulate loss\n","            loss.backward()                     # Backward pass\n","\n","            optimizer.step()                    # Update params\n","\n","            if step % 10 == 0:\n","                # Progress update every 40 batches                \n","                print('  Batch {:>5,}  of  {:>5,}    |    Elapsed: {:.0f}s.'.format(step, len(train_batches), time.time() - t0))\n","                print(f'  Loss = {loss.item():.2f}')\n","\n","            torch.cuda.empty_cache()            # Clear GPU cache to avoid memory issues\n","\n","        # Compute and store avg loss\n","        avg_train_loss = total_loss / len(X)\n","        losses.append(avg_train_loss)\n","\n","        print(\"\")\n","        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","        print(\"  Training epoch took: {:.0f}s\".format(time.time() - t0))\n","\n","        validation(val_batches) \n","\n","    print(\"\")\n","    print(\"Training complete!\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"510faefa-1882-4aa8-b9d6-dcf2a14f2d54","executionInfo":{"status":"error","timestamp":1581877852775,"user_tz":0,"elapsed":13388,"user":{"displayName":"Alex Gaskell","photoUrl":"","userId":"12223185016372309013"}},"id":"HySBDk2P9aco","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["torch.cuda.empty_cache()\n","# Model to train\n","model = BertRegressor(batches_train[0][0].shape, bert_config, device)\n","\n","# Optimizer\n","optimizer = AdamW(model.params(), lr=LR, eps=1e-8)\n","\n","torch.cuda.empty_cache()\n","train(model, batches_train, batches_val, epochs=EPOCHS)\n","torch.cuda.empty_cache()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 2 ========\n","Training...\n","  Batch     0  of    350    |    Elapsed: 1s.\n","  Loss = 0.66\n","  Batch    10  of    350    |    Elapsed: 7s.\n","  Loss = 0.79\n","  Batch    20  of    350    |    Elapsed: 13s.\n","  Loss = 0.88\n","  Batch    30  of    350    |    Elapsed: 20s.\n","  Loss = 0.45\n","  Batch    40  of    350    |    Elapsed: 26s.\n","  Loss = 0.84\n","  Batch    50  of    350    |    Elapsed: 33s.\n","  Loss = 0.69\n","  Batch    60  of    350    |    Elapsed: 39s.\n","  Loss = 0.97\n","  Batch    70  of    350    |    Elapsed: 46s.\n","  Loss = 1.32\n","  Batch    80  of    350    |    Elapsed: 52s.\n","  Loss = 0.92\n","  Batch    90  of    350    |    Elapsed: 59s.\n","  Loss = 0.87\n","  Batch   100  of    350    |    Elapsed: 65s.\n","  Loss = 0.78\n","  Batch   110  of    350    |    Elapsed: 71s.\n","  Loss = 0.75\n","  Batch   120  of    350    |    Elapsed: 78s.\n","  Loss = 0.80\n","  Batch   130  of    350    |    Elapsed: 84s.\n","  Loss = 1.02\n","  Batch   140  of    350    |    Elapsed: 91s.\n","  Loss = 0.62\n","  Batch   150  of    350    |    Elapsed: 97s.\n","  Loss = 1.06\n","  Batch   160  of    350    |    Elapsed: 104s.\n","  Loss = 0.87\n","  Batch   170  of    350    |    Elapsed: 111s.\n","  Loss = 1.07\n","  Batch   180  of    350    |    Elapsed: 117s.\n","  Loss = 0.95\n","  Batch   190  of    350    |    Elapsed: 124s.\n","  Loss = 1.03\n","  Batch   200  of    350    |    Elapsed: 130s.\n","  Loss = 0.83\n","  Batch   210  of    350    |    Elapsed: 137s.\n","  Loss = 0.78\n","  Batch   220  of    350    |    Elapsed: 144s.\n","  Loss = 0.75\n","  Batch   230  of    350    |    Elapsed: 150s.\n","  Loss = 0.74\n","  Batch   240  of    350    |    Elapsed: 157s.\n","  Loss = 0.68\n","  Batch   250  of    350    |    Elapsed: 163s.\n","  Loss = 0.70\n","  Batch   260  of    350    |    Elapsed: 170s.\n","  Loss = 1.14\n","  Batch   270  of    350    |    Elapsed: 177s.\n","  Loss = 1.05\n","  Batch   280  of    350    |    Elapsed: 183s.\n","  Loss = 0.95\n","  Batch   290  of    350    |    Elapsed: 190s.\n","  Loss = 0.94\n","  Batch   300  of    350    |    Elapsed: 196s.\n","  Loss = 0.90\n","  Batch   310  of    350    |    Elapsed: 203s.\n","  Loss = 0.87\n","  Batch   320  of    350    |    Elapsed: 210s.\n","  Loss = 1.24\n","  Batch   330  of    350    |    Elapsed: 216s.\n","  Loss = 0.71\n","  Batch   340  of    350    |    Elapsed: 223s.\n","  Loss = 0.76\n","\n","  Average training loss: 19.09\n","  Training epoch took: 229s\n","\n","Running Validation...\n","|  Correlation: 0.04     |\n","|  Validation took: 7.103529 s |\n","\n","======== Epoch 2 / 2 ========\n","Training...\n","  Batch     0  of    350    |    Elapsed: 1s.\n","  Loss = 0.48\n","  Batch    10  of    350    |    Elapsed: 7s.\n","  Loss = 0.74\n","  Batch    20  of    350    |    Elapsed: 14s.\n","  Loss = 0.81\n","  Batch    30  of    350    |    Elapsed: 20s.\n","  Loss = 0.36\n","  Batch    40  of    350    |    Elapsed: 27s.\n","  Loss = 0.76\n","  Batch    50  of    350    |    Elapsed: 33s.\n","  Loss = 0.59\n","  Batch    60  of    350    |    Elapsed: 40s.\n","  Loss = 0.92\n","  Batch    70  of    350    |    Elapsed: 47s.\n","  Loss = 1.18\n","  Batch    80  of    350    |    Elapsed: 53s.\n","  Loss = 0.81\n","  Batch    90  of    350    |    Elapsed: 60s.\n","  Loss = 0.75\n","  Batch   100  of    350    |    Elapsed: 66s.\n","  Loss = 0.58\n","  Batch   110  of    350    |    Elapsed: 73s.\n","  Loss = 0.74\n","  Batch   120  of    350    |    Elapsed: 79s.\n","  Loss = 0.49\n","  Batch   130  of    350    |    Elapsed: 86s.\n","  Loss = 0.65\n","  Batch   140  of    350    |    Elapsed: 93s.\n","  Loss = 0.47\n","  Batch   150  of    350    |    Elapsed: 99s.\n","  Loss = 0.81\n","  Batch   160  of    350    |    Elapsed: 106s.\n","  Loss = 0.73\n","  Batch   170  of    350    |    Elapsed: 112s.\n","  Loss = 0.74\n","  Batch   180  of    350    |    Elapsed: 119s.\n","  Loss = 0.53\n","  Batch   190  of    350    |    Elapsed: 126s.\n","  Loss = 0.56\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XxgLLx-hwOxH","colab_type":"code","colab":{}},"source":["\n","# model = BertRegressor(batches_train[0][0].shape, bert_config, device)\n","\n","# torch.cuda.empty_cache()\n","\n","# X = batches_train[0][0][:10].to(device, torch.long)\n","# mask = batches_train[0][1][:10].to(device)\n","# y = batches_train[0][2][:10].to(device)\n","\n","# optimizer = AdamW(model.params(), lr=LR, eps=1e-8)\n","\n","# with torch.no_grad():\n","#     X_ = model.bert(X)[0].view(X.shape[0], -1)\n","\n","# losses = []\n","\n","# for i in range(len(batches_train)):\n","\n","#     X = batches_train[i][0][:].to(device, torch.long)\n","#     mask = batches_train[i][1][:].to(device)\n","#     y = batches_train[i][2][:].to(device)\n","\n","#     model.zero_grad()\n","\n","#     X_ = model.bert(X)[0].view(X.shape[0], -1)\n","#     preds = model.linear(X_)\n","    \n","#     loss = model.loss(preds, y)\n","#     loss.backward()\n","#     print(i, '   ',loss)\n","#     optimizer.step()\n","#     losses.append(loss)\n","#     torch.cuda.empty_cache()\n","\n","# plt.plot(losses)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Q9X89fixYMx","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4k1zP2FvxPA8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uyZOC5rRxPHK","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kyME3nBYxPNj","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"acCovitAxPSx","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eZQ7joMLxPXi","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MRckS7e7xPc0","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QKf1nBE3xPiL","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UZxNAxikxPnq","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P6tazwxpxPsw","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bOFv_zh5xPx2","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"__5QoNXSsz38","colab_type":"code","colab":{}},"source":["# Get all of the model's parameters as a list of tuples.\n","params = list(model.named_parameters())\n","\n","print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n","\n","print('==== Embedding Layer ====\\n')\n","\n","for p in params[0:5]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== First Transformer ====\\n')\n","\n","for p in params[5:21]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== Output Layer ====\\n')\n","\n","for p in params[-4:]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sWGcuHc2s2J8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}