{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert_multilingual.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github","colab_type":"text"},"source":["<a href=\"https://colab.research.google.com/github/alexgaskell10/NLP_Translation/blob/master/notebooks/stuff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","metadata":{"id":"hpAd9yKPDnlI","colab_type":"code","outputId":"0683c879-bce0-4654-c359-06f0d9c68955","executionInfo":{"status":"ok","timestamp":1581884148647,"user_tz":0,"elapsed":3694,"user":{"displayName":"Alex Gaskell","photoUrl":"","userId":"12223185016372309013"}},"colab":{"base_uri":"https://localhost:8080/","height":378}},"source":["! pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.4.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n","Requirement already satisfied: tokenizers==0.0.11 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.11)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8yKxJ_GREs_C","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":62},"outputId":"fec0cfda-78dc-43e7-8a23-a60b92b68759","executionInfo":{"status":"ok","timestamp":1581884150494,"user_tz":0,"elapsed":5514,"user":{"displayName":"Alex Gaskell","photoUrl":"","userId":"12223185016372309013"}}},"source":["import torch\n","from torch.nn import Linear, ModuleList\n","from torch import optim\n","import torch.nn.functional as F\n","from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n","from sklearn.model_selection import train_test_split\n","import random\n","import time\n","import numpy as np\n","from scipy.stats import pearsonr\n","import matplotlib.pyplot as plt\n","import os\n","torch.set_printoptions(2)"],"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"scs7ICZrPFcs","colab_type":"code","colab":{}},"source":["# # ############ ENGLISH - CHINESE ############\n","\n","# # Download and unzip the data\n","# from os.path import exists\n","# if not exists('enzh_data.zip'):\n","#     !wget -O enzh_data.zip https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n","#     !unzip enzh_data.zip\n","\n","# # Load data into variables\n","# with open(\"./train.enzh.src\", \"r\") as ende_src:\n","#     en_corpus = ende_src.read().split('\\n')[:-1]\n","# with open(\"./train.enzh.mt\", \"r\") as ende_src:\n","#     de_corpus = ende_src.read().split('\\n')[:-1]\n","# with open(\"./train.enzh.scores\", \"r\") as ende_src:\n","#     scores = [float(x) for x in ende_src.read().split('\\n')[:-1]]\n","\n","# print(en_corpus[0])\n","# print(de_corpus[0])\n","# print(scores[0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yotBQbegEtVA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":98},"outputId":"43360e46-7e80-461d-a0ad-886b02d0b354","executionInfo":{"status":"ok","timestamp":1581884150496,"user_tz":0,"elapsed":5477,"user":{"displayName":"Alex Gaskell","photoUrl":"","userId":"12223185016372309013"}}},"source":["########### ENGLISH - GERMAN ############\n","\n","# Download and unzip the data\n","from os.path import exists\n","if not exists('ende_data.zip'):\n","    !wget -O ende_data.zip https://competitions.codalab.org/my/datasets/download/c748d2c0-d6be-4e36-9f12-ca0e88819c4d\n","    !unzip ende_data.zip\n","\n","# Load data into variables\n","with open(\"./train.ende.src\", \"r\") as ende_src:\n","    en_corpus = ende_src.read().split('\\n')[:-1]\n","with open(\"./train.ende.mt\", \"r\") as ende_src:\n","    de_corpus = ende_src.read().split('\\n')[:-1]\n","with open(\"./train.ende.scores\", \"r\") as ende_src:\n","    scores = [float(x) for x in ende_src.read().split('\\n')[:-1]]\n","\n","# Load data into variables\n","with open(\"./test.ende.src\", \"r\") as ende_src:\n","    en_corpus_test = ende_src.read().split('\\n')[:-1]\n","with open(\"./test.ende.mt\", \"r\") as ende_src:\n","    de_corpus_test = ende_src.read().split('\\n')[:-1]\n","\n","\n","print(en_corpus[0])\n","print(de_corpus[0])\n","print(scores[0])\n","\n","print(en_corpus_test[0])\n","print(de_corpus_test[0])"],"execution_count":4,"outputs":[{"output_type":"stream","text":["José Ortega y Gasset visited Husserl at Freiburg in 1934.\n","1934 besuchte José Ortega y Gasset Husserl in Freiburg.\n","1.1016968715664406\n","The Sultan appoints judges, and can grant pardons and commute sentences.\n","Der Sultan ernennt Richter und kann Begnadigungen und Pendelstrafen gewähren.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"D9BO2HwcgWM9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":98},"outputId":"516ee2f9-f2cb-4709-874f-9eaeda22b30e","executionInfo":{"status":"ok","timestamp":1581884153508,"user_tz":0,"elapsed":8466,"user":{"displayName":"Alex Gaskell","photoUrl":"","userId":"12223185016372309013"}}},"source":["! ls"],"execution_count":5,"outputs":[{"output_type":"stream","text":["dev.ende.mt\t dev.enzh.src\ttest.ende.mt   train.ende.scores\n","dev.ende.scores  drive\t\ttest.ende.src  train.ende.src\n","dev.ende.src\t ende_data.zip\ttest.enzh.mt   train.enzh.mt\n","dev.enzh.mt\t enzh_data.zip\ttest.enzh.src  train.enzh.scores\n","dev.enzh.scores  sample_data\ttrain.ende.mt  train.enzh.src\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XizhBzbFtuic","colab_type":"code","outputId":"d6677101-3629-4c43-deef-681228567d78","executionInfo":{"status":"ok","timestamp":1581884153510,"user_tz":0,"elapsed":8446,"user":{"displayName":"Alex Gaskell","photoUrl":"","userId":"12223185016372309013"}},"colab":{"base_uri":"https://localhost:8080/","height":284}},"source":["plt.plot(sorted(scores))\n","plt.ylim([-2,2])"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(-2, 2)"]},"metadata":{"tags":[]},"execution_count":6},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"rvu5GAlL5-wc","colab_type":"code","colab":{}},"source":["class Preprocessor:\n","    def __init__(self, device, bert_config):\n","        self.device = device\n","        self.tokenizer = BertTokenizer.from_pretrained(bert_config)\n","\n","    def get_input_tensor(self, en_corpus, de_corpus):\n","        # Convert input sequences to correct format\n","        N = len(en_corpus)\n","\n","        # Tokenize corpora\n","        inputs_en, inputs_de = [],[]\n","        max_len_en, max_len_de = 0,0\n","            \n","        for i in range(N):\n","            # English\n","            seq = en_corpus[i][:-1]\n","            input_ids = torch.tensor([self.tokenizer.encode(seq, add_special_tokens=True)])\n","            inputs_en.append(input_ids)\n","            if input_ids.shape[-1] > max_len_en:\n","                max_len_en = input_ids.shape[-1]\n","\n","            # German\n","            seq = de_corpus[i][:-1]\n","            input_ids = torch.tensor([self.tokenizer.encode(seq, add_special_tokens=True)])\n","            inputs_de.append(input_ids)\n","            if input_ids.shape[-1] > max_len_de:\n","                max_len_de = input_ids.shape[-1]\n","\n","        # Combine tokens into single tensor\n","        inp_tensor = torch.zeros((N, max_len_en + max_len_de - 2))      # <-- -2 because special tokens are not necessary at beginning of German sequence\n","\n","        for i in range(N):\n","            # Add English tokens\n","            en_tokens = inputs_en[i].squeeze()\n","            inp_tensor[i, : len(en_tokens)] = en_tokens\n","\n","            # Add German tokens\n","            de_tokens = inputs_de[i][:,2:].squeeze()      # <-- ignore first 2 tokens as these are special tokens and unnecessary in this case\n","            inp_tensor[i, max_len_en : max_len_en + len(de_tokens)] = de_tokens\n","\n","        return inp_tensor\n","\n","    def attention_mask(self, list_input_tensor):\n","        list_attention_masks = []\n","        for input_tensor in list_input_tensor:\n","            attention_mask = torch.zeros((input_tensor.shape))\n","            attention_mask[input_tensor != 0] = 1\n","            list_attention_masks.append(attention_mask)\n","\n","        return list_attention_masks\n","\n","    def get_batches(self, inp_tensor, scores, BATCH_N):\n","        inp_tensor = inp_tensor\n","        scores = torch.tensor(scores).view(-1,1)\n","\n","        # Split data into train, test and val batches\n","        inp_tensor_batches = torch.split(inp_tensor, BATCH_N)\n","        scores_batches = torch.split(scores, BATCH_N)\n","        X_train_val, X_test, y_train_val, y_test = train_test_split(inp_tensor_batches, scores_batches, random_state=1, test_size=0.1)\n","        X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=1, test_size=1/9)\n","        print(len(X_train), X_train[0].shape)\n","\n","        # Create attention masks\n","        X_train_mask = self.attention_mask(X_train)\n","        X_val_mask = self.attention_mask(X_val) \n","        X_test_mask = self.attention_mask(X_test)\n","        print(len(X_train_mask), X_train_mask[0].shape)\n","\n","        # Batch X, mask and y together\n","        batches_train = [(X, mask, y) for X,mask,y in zip(X_train, X_train_mask, y_train)]\n","        batches_val = [(X, mask, y) for X,mask,y in zip(X_val, X_val_mask, y_val)]\n","        batches_test = [(X, mask, y) for X,mask,y in zip(X_test, X_test_mask, y_test)]\n","\n","        return batches_train, batches_val, batches_test\n","\n","    def get_test_batches(self, inp_tensor, BATCH_N):\n","        inp_tensor = inp_tensor\n","\n","        # Split data into train, test and val batches\n","        X = torch.split(inp_tensor, BATCH_N)\n","\n","        # Create attention masks\n","        X_mask = self.attention_mask(X)\n","\n","        # Batch X and mask together\n","        batches = [(x, mask) for x,mask in zip(X, X_mask)]\n","\n","        return batches\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LWG59VN-23aV","colab_type":"code","colab":{}},"source":["class BertRegressor:\n","    def __init__(self, input_shape, bert_config, device):\n","        self.device = device\n","        self.bert = BertModel.from_pretrained(bert_config).to(device)\n","        self.layers = ModuleList([\n","            # Linear(in_features=input_shape[-1], out_features=1, bias=True).to(device),\n","            # Linear(in_features=input_shape[-1]*768, out_features=100, bias=True).to(device),            # <-- 768 is the dim of Bert embedding\n","            Linear(in_features=768, out_features=100, bias=True).to(device),            # <-- 768 is the dim of Bert embedding\n","            Linear(in_features=100, out_features=1, bias=True).to(device),\n","        ])\n","\n","    def forward_1(self, X, attention_mask=None):\n","        if attention_mask is not None:\n","            X = self.bert(X, attention_mask)[0].view(X.shape[0], -1)    \n","        else:\n","            X = self.bert(X)[0].view(X.shape[0], -1)\n","\n","        X = self.layers[0](X)\n","        for l in self.layers[1:]:\n","            X = F.relu(X)\n","            X = l(X)\n","\n","        return X\n","\n","    # def forward_2(self, X, attention_mask=None):\n","    #     if attention_mask is not None:\n","    #         X = self.bert(X, attention_mask)[0][:,:,0]\n","    #         print(X.shape)\n","    #     else:\n","    #         X = self.bert(X)[0][:,:,0]\n","\n","    #     X = self.layers[0](X)\n","    #     for l in self.layers[1:]:\n","    #         X = F.relu(X)\n","    #         X = l(X)\n","\n","    #     return X\n","\n","    def forward_2(self, X, attention_mask=None):\n","        if attention_mask is not None:\n","            X = self.bert(X, attention_mask)[1]\n","        else:\n","            X = self.bert(X)[1]\n","\n","        X = self.layers[0](X)\n","        for l in self.layers[1:]:\n","            X = F.relu(X)\n","            X = l(X)\n","\n","        return X\n","\n","    def loss(self, scores, pred_scores):\n","        rmse = (((pred_scores - scores)**2).mean())**0.5\n","        return rmse\n","\n","    def check_r(self, y_pred, y):\n","        return pearsonr(y_pred.cpu().squeeze(), y.cpu().squeeze())[0]\n","\n","    def zero_grad(self):\n","        self.bert.zero_grad()\n","        for l in self.layers:\n","            l.zero_grad()\n","\n","    def params(self):\n","        params = list(self.bert.parameters())\n","        for l in self.layers:\n","            params.extend(list(l.parameters()))\n","        return params\n","\n","    def __call__(self, X, attention_mask=None):\n","        return self.forward_2(X, attention_mask)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Rp7NCpb-9NL-","outputId":"59b2cbcf-fe29-45b4-df11-bc51726dc35b","executionInfo":{"status":"ok","timestamp":1581884158584,"user_tz":0,"elapsed":13473,"user":{"displayName":"Alex Gaskell","photoUrl":"","userId":"12223185016372309013"}},"colab":{"base_uri":"https://localhost:8080/","height":49}},"source":["USE_GPU = True\n","\n","if USE_GPU and torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","\n","bert_config = 'bert-base-multilingual-cased'\n","BATCH_N = 16\n","EPOCHS = 2\n","LR = 2e-5\n","\n","# Set seeds\n","seed_val = 111\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# Preprocessing\n","preprocessor = Preprocessor(device, bert_config)\n","input_tensor = preprocessor.get_input_tensor(en_corpus, de_corpus)\n","batches_train, batches_val, batches_test = preprocessor.get_batches(input_tensor, scores, BATCH_N)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["350 torch.Size([16, 132])\n","350 torch.Size([16, 132])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4m8jwjBaCsAJ","colab_type":"code","colab":{}},"source":["# torch.cuda.empty_cache()\n","# model = BertRegressor(batches_train[0][0].shape, bert_config, device)\n","\n","# X = batches_train[0][0].to(device, dtype=torch.long)\n","# mask = batches_train[0][1].to(device)\n","# # print(X.device, mask.device, X.requires_grad)\n","# print(model(X, mask).shape)\n","# # print(model.bert.forward(X)[0][:,:,0].shape)\n","# torch.cuda.empty_cache()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8306qtg603CS","colab_type":"code","colab":{}},"source":["def validation(val_batches):\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","    eval_loss = 0\n","    correlations = []\n","\n","    for batch in val_batches:        \n","\n","        # Untie batch and put on GPU\n","        X = batch[0].to(device=device, dtype=torch.long)\n","        mask = batch[1].to(device)\n","        y = batch[2].to(device)\n","\n","        with torch.no_grad():        \n","            y_pred = model(X)                      \n","                \n","        # Compute and record batch accuracy\n","        corr = model.check_r(y_pred, y)\n","        correlations.append(corr)\n","\n","    # Report the final accuracy for this validation run.\n","    print(f\"|  Correlation: {np.mean(correlations):.2f}     |\")\n","    print(f\"|  Validation took: {time.time() - t0:0f} s |\")\n","\n","    torch.cuda.empty_cache()\n","\n","\n","def train(model, train_batches, val_batches, epochs):\n","\n","    losses = []\n","    \n","    for epoch_i in range(epochs):\n","        \n","        print(\"\")\n","        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","        print('Training...')\n","\n","        t0 = time.time()\n","        total_loss = 0\n","\n","        for step, batch in enumerate(train_batches):\n","\n","            # Untie batch and put on GPU\n","            X = batch[0].to(device=device, dtype=torch.long)\n","            mask = batch[1].to(device)\n","            y = batch[2].to(device)\n","\n","            model.zero_grad()                   # Reset grads\n","            y_pred = model(X, mask)             # Forward pass\n","            loss = model.loss(y, y_pred)        # Compute loss\n","            total_loss += loss.item()           # Accumulate loss\n","            loss.backward()                     # Backward pass\n","\n","            optimizer.step()                    # Update params\n","\n","            if step % 10 == 0:\n","                # Progress update every 40 batches                \n","                print('  Batch {:>5,}  of  {:>5,}    |    Elapsed: {:.0f}s.'.format(step, len(train_batches), time.time() - t0))\n","                print(f'  Loss = {loss.item():.2f}')\n","\n","            torch.cuda.empty_cache()            # Clear GPU cache to avoid memory issues\n","\n","        # Compute and store avg loss\n","        avg_train_loss = total_loss / len(X)\n","        losses.append(avg_train_loss)\n","\n","        print(\"\")\n","        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","        print(\"  Training epoch took: {:.0f}s\".format(time.time() - t0))\n","\n","        validation(val_batches) \n","\n","    print(\"\")\n","    print(\"Training complete!\")\n","\n","\n","def writeScores(method_name, scores):\n","    fn = \"predictions.txt\"\n","    print(\"\")\n","    with open(fn, 'w') as output_file:\n","        for idx,x in enumerate(scores):\n","            out =  metrics[idx]+\":\"+str(\"{0:.2f}\".format(x))+\"\\n\"\n","            print(out)\n","            # output_file.write(f\"{x}\\n\")\n","\n","\n","def get_test_preds(model, en_test, de_test):\n","    torch.cuda.empty_cache()\n","    # Preprocessing\n","    preprocessor = Preprocessor(device, bert_config)\n","    input_tensor = preprocessor.get_input_tensor(en_test, de_test)\n","    batches = preprocessor.get_test_batches(input_tensor, BATCH_N=32)\n","    # mask = torch.cat(preprocessor.attention_mask(input_tensor), dim=0).view(input_tensor.shape).to(device).long()\n","    # print(mask.shape)\n","    # print(input_tensor.shape)\n","    y_preds = []\n","    for batch in batches:\n","        # Untie batch and put on GPU\n","        X = batch[0].to(device=device, dtype=torch.long)\n","        mask = batch[1].to(device)\n","\n","        model.zero_grad()                   # Reset grads\n","        y_pred = model(X, mask)             # Forward pass\n","\n","        y_preds.append(y_pred)\n","\n","    print(y_preds)\n","    # writeScores(y_pred)\n","    # torch.cuda.empty_cache()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UikgOnHsjRzL","colab_type":"code","colab":{}},"source":["# dir_path = '/content/drive/My Drive/Documents/Imperial/NLP/NLP_Translation/notebooks/models/'\n","\n","# try:\n","#     print(os.listdir(dir_path))\n","# except:\n","#     from google.colab import drive\n","#     drive.mount('/content/drive')\n","\n","# state_dict = torch.load(\n","#     dir_path + '/bert_1.pth',\n","#     map_location=lambda storage, loc: storage.cuda(device),\n","# )\n","# model.load_state_dict(state_dict)\n","\n","# model = model.to(device=device)\n","# get_test_preds(model, en_corpus_test, de_corpus_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"cc7fb95a-2244-4ab6-bdd5-4ab9118ebd46","executionInfo":{"status":"error","timestamp":1581885170118,"user_tz":0,"elapsed":1587,"user":{"displayName":"Alex Gaskell","photoUrl":"","userId":"12223185016372309013"}},"id":"HySBDk2P9aco","colab":{"base_uri":"https://localhost:8080/","height":208}},"source":["# torch.cuda.empty_cache()\n","# # Model to train\n","# model = BertRegressor(batches_train[0][0].shape, bert_config, device)\n","\n","# # Optimizer\n","# optimizer = AdamW(model.params(), lr=LR, eps=1e-8)\n","\n","# torch.cuda.empty_cache()\n","# train(model, batches_train, batches_val, epochs=EPOCHS)\n","# torch.cuda.empty_cache()\n","\n","# Directory for saving reconstructions + models\n","dir_path = '/content/drive/My Drive/Documents/Imperial/NLP/NLP_Translation/notebooks/models/'\n","\n","try:\n","    print(os.listdir(dir_path))\n","except:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","torch.save(model.state_dict(), dir_path + '/bert_1.pth')"],"execution_count":14,"outputs":[{"output_type":"stream","text":["[]\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-1ed4899335f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/bert_1.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'BertRegressor' object has no attribute 'state_dict'"]}]},{"cell_type":"code","metadata":{"id":"LN3NtkF4kPxw","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FVss_RLBkFei","colab_type":"code","colab":{}},"source":["\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XWnNUR0Gku_9","colab_type":"code","colab":{}},"source":["# from google.colab import files\n","# from zipfile import ZipFile\n","\n","# writeScores(\"SVR\", predictions)\n","\n","# with ZipFile(\"en-de_svr.zip\",\"w\") as newzip:\n","# \tnewzip.write(\"predictions.txt\")\n"," \n","# files.downloa('en-de_svr.zip') "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XxgLLx-hwOxH","colab_type":"code","colab":{}},"source":["\n","# model = BertRegressor(batches_train[0][0].shape, bert_config, device)\n","\n","# torch.cuda.empty_cache()\n","\n","# X = batches_train[0][0][:10].to(device, torch.long)\n","# mask = batches_train[0][1][:10].to(device)\n","# y = batches_train[0][2][:10].to(device)\n","\n","# optimizer = AdamW(model.params(), lr=LR, eps=1e-8)\n","\n","# with torch.no_grad():\n","#     X_ = model.bert(X)[0].view(X.shape[0], -1)\n","\n","# losses = []\n","\n","# for i in range(len(batches_train)):\n","\n","#     X = batches_train[i][0][:].to(device, torch.long)\n","#     mask = batches_train[i][1][:].to(device)\n","#     y = batches_train[i][2][:].to(device)\n","\n","#     model.zero_grad()\n","\n","#     X_ = model.bert(X)[0].view(X.shape[0], -1)\n","#     preds = model.linear(X_)\n","    \n","#     loss = model.loss(preds, y)\n","#     loss.backward()\n","#     print(i, '   ',loss)\n","#     optimizer.step()\n","#     losses.append(loss)\n","#     torch.cuda.empty_cache()\n","\n","# plt.plot(losses)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Q9X89fixYMx","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4k1zP2FvxPA8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uyZOC5rRxPHK","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kyME3nBYxPNj","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"acCovitAxPSx","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eZQ7joMLxPXi","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MRckS7e7xPc0","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QKf1nBE3xPiL","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UZxNAxikxPnq","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P6tazwxpxPsw","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bOFv_zh5xPx2","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"__5QoNXSsz38","colab_type":"code","colab":{}},"source":["# Get all of the model's parameters as a list of tuples.\n","params = list(model.named_parameters())\n","\n","print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n","\n","print('==== Embedding Layer ====\\n')\n","\n","for p in params[0:5]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== First Transformer ====\\n')\n","\n","for p in params[5:21]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== Output Layer ====\\n')\n","\n","for p in params[-4:]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sWGcuHc2s2J8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}