{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "torch.set_printoptions(4)\n",
    "from torch import optim\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For downloading and saving model locally.\n",
    "\n",
    "# tokenizer_en = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "# model_en = BertModel.from_pretrained('..\\..\\Dump Files\\models\\BERT_cased_en')\n",
    "\n",
    "# tokenizer_de = BertTokenizer.from_pretrained('bert-base-german-cased')\n",
    "# model_de = BertModel.from_pretrained('..\\..\\Dump Files\\models\\BERT_cased_de')\n",
    "\n",
    "# model_en.save_pretrained('..\\..\\Dump Files\\models\\BERT_cased_en')\n",
    "\n",
    "# model_de.save_pretrained('..\\..\\Dump Files\\models\\BERT_cased_de')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "#model = BertModel.from_pretrained('..\\..\\Dump Files\\models\\BERT_cased_multi')\n",
    "\n",
    "#model.save_pretrained('..\\..\\Dump Files\\models\\BERT_cased_multi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "\n",
    "def load_data(set_name):\n",
    "    \"\"\"\n",
    "    set name: \"train\", \"dev\", \"test\"\n",
    "    \"\"\"\n",
    "\n",
    "# Load train data into variables\n",
    "    with open(os.path.join(\"..\", \"data\", \"{}.ende.src\".format(set_name)), \"r\", encoding=\"utf8\") as ende_src:\n",
    "        en_set = ende_src.read().split('\\n')\n",
    "    with open(os.path.join(\"..\", \"data\", \"{}.ende.mt\".format(set_name)), \"r\", encoding=\"utf8\") as ende_mt:\n",
    "        de_set = ende_mt.read().split('\\n')\n",
    "    \n",
    "\n",
    "    del en_set[len(en_set)-1]\n",
    "    del de_set[len(de_set)-1]\n",
    "\n",
    "    \n",
    "    return en_set, de_set\n",
    "\n",
    "def load_scores(set_name):\n",
    "    \n",
    "    with open(os.path.join(\"..\", \"data\", \"{}.ende.scores\".format(set_name)), \"r\", encoding=\"utf8\") as ende_scores:\n",
    "        scores = [float(x) for x in ende_scores.read().split('\\n')[:-1]]\n",
    "        \n",
    "    #del scores[len(scores)-1]\n",
    "    print(len(scores))\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "en_train, de_train = load_data(\"train\")\n",
    "scores_train = load_scores(\"train\")\n",
    "\n",
    "en_val, de_val = load_data(\"dev\")\n",
    "scores_val = load_scores(\"dev\")\n",
    "\n",
    "en_test, de_test = load_data(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X, tokenizer):\n",
    "    inputs = []\n",
    "    max_len_inps = 0\n",
    "\n",
    "    # Tokenize\n",
    "    for i in range(len(X)-1):\n",
    "        seq = X[i][:-1]\n",
    "        # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n",
    "        input_ids = torch.tensor([tokenizer.encode(seq, add_special_tokens=True)])  \n",
    "        inputs.append(input_ids)\n",
    "        if input_ids.shape[-1] > max_len_inps:\n",
    "            max_len_inps = input_ids.shape[-1]\n",
    "            \n",
    "    # Convert to tensor\n",
    "    inp_tensor = torch.zeros((len(X), max_len_inps))\n",
    "    \n",
    "    for i in range(len(inputs)):\n",
    "    # Add tokens\n",
    "        tokens = inputs[i].squeeze()\n",
    "        inp_tensor[i, : len(tokens)] = tokens\n",
    "        \n",
    "    print(inp_tensor.shape)\n",
    "    return inp_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7000, 65])\n"
     ]
    }
   ],
   "source": [
    "en_inputs = preprocess(en_train, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7000, 69])\n"
     ]
    }
   ],
   "source": [
    "de_inputs = preprocess(de_train, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 48])\n",
      "torch.Size([1000, 54])\n"
     ]
    }
   ],
   "source": [
    "en_val_inputs = preprocess(en_val, tokenizer)\n",
    "de_val_inputs = preprocess(de_val, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embeddings(model, inp_tensor):\n",
    "\n",
    "    model = model.to(device='cuda')\n",
    "    inp_tensor = inp_tensor.to(device='cuda', dtype=torch.long)\n",
    "\n",
    "    batches = torch.split(inp_tensor, 250, dim=0)\n",
    "    list_bert_embs = []\n",
    "    for X in batches:\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = model(X)[0]    # <-- take word embeddings ([1] gives sentence embeddings)\n",
    "\n",
    "        list_bert_embs.append(last_hidden_states)\n",
    "\n",
    "        #print(last_hidden_states.shape)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    bert_embs = torch.cat(list_bert_embs, dim=0)\n",
    "    \n",
    "    # Now, slice the tensor to only keep the [CLS] embedding\n",
    "\n",
    "    print(bert_embs.shape)\n",
    "    \n",
    "    return bert_embs[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert_embs_en = get_sentence_embeddings(model_en, en_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert_embs_en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert_embs_de = get_sentence_embeddings(model_de, de_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(BertRegressor, self).__init__()\n",
    "        \n",
    "        self.device = \"cuda\"\n",
    "        self.bert = BertModel.from_pretrained('..\\..\\Dump Files\\models\\BERT_cased_multi').to(\"cuda\")\n",
    "        \n",
    "        \n",
    "        self.linear1 = nn.Linear(in_features = 2*768,                   # <-- 768 is the dim of Bert embedding\n",
    "                             out_features = 256, bias = True).to(\"cuda\")\n",
    "        self.bn1 = nn.BatchNorm1d(256).to(\"cuda\")\n",
    "        \n",
    "        self.linear2 = nn.Linear(in_features = 256,                  \n",
    "                             out_features = 128, bias = True).to(\"cuda\")\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm1d(128).to(\"cuda\")\n",
    "        \n",
    "        self.linear3 = nn.Linear(in_features = 128,                  \n",
    "                             out_features = 1, bias = True).to(\"cuda\")\n",
    "\n",
    "    def forward(self, X_en, X_de, attention_mask_en=None, attention_mask_de=None):\n",
    "        with torch.no_grad():\n",
    "            if attention_mask_en is not None:\n",
    "                X_en = self.bert(X_en, attention_mask_en)[1]   \n",
    "            else:\n",
    "                X_en = self.bert(X_en)[1]\n",
    "\n",
    "            if attention_mask_de is not None:\n",
    "                X_de = self.bert(X_de, attention_mask_de)[1]  \n",
    "            else:\n",
    "                X_de = self.bert(X_de)[1]\n",
    "        \n",
    "#         X_en_CLS = X_en[:,0,:]\n",
    "#         X_de_CLS = X_de[:,0,:]\n",
    "#         print(X_en.shape)\n",
    "\n",
    "        X = torch.cat((X_en, X_de), 1) # concatenate along 1st dimension\n",
    "        \n",
    "        X = F.tanh(self.linear1(X))\n",
    "        #X = self.bn1(X)\n",
    "        X = F.tanh(self.linear2(X))\n",
    "        #X = self.bn2(X)\n",
    "        preds = self.linear3(X)\n",
    "        \n",
    "        #print(self.bert.parameters())\n",
    "        \n",
    "        return preds\n",
    "\n",
    "    def loss(self, scores, pred_scores):\n",
    "        rmse = (((pred_scores - scores)**2).mean())**0.5\n",
    "        return rmse\n",
    "\n",
    "    def check_r(self, y_pred, y):\n",
    "        return pearsonr(y_pred.cpu().squeeze(), y.cpu().squeeze())[0]\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.bert.zero_grad()\n",
    "        #self.bert_de.zero_grad()\n",
    "        self.linear1.zero_grad()\n",
    "        self.linear2.zero_grad()\n",
    "        self.linear3.zero_grad()\n",
    "        self.bn1.zero_grad()\n",
    "        self.bn2.zero_grad()\n",
    "\n",
    "    def params(self):\n",
    "        params = list(self.bert.parameters()) + list(self.linear1.parameters()) + list(self.linear2.parameters()) + list(self.linear3.parameters())\n",
    "        #    list(self.bn1.parameters()) + \n",
    "\n",
    "        #list(self.bn2.parameters()) + \n",
    "        \n",
    "        return params\n",
    "\n",
    "    def __call__(self, X_en, X_de, attention_mask_en=None, attention_mask_de=None):\n",
    "        return self.forward(X_en, X_de, attention_mask_en, attention_mask_de)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_mask(list_input_tensor):\n",
    "    list_attention_masks = []\n",
    "    for input_tensor in list_input_tensor:\n",
    "        attention_mask = torch.zeros((input_tensor.shape))\n",
    "        attention_mask[input_tensor != 0] = 1\n",
    "        list_attention_masks.append(attention_mask)\n",
    "\n",
    "    return list_attention_masks\n",
    "\n",
    "def get_batches(inp_tensor, scores, BATCH_N):\n",
    "    inp_tensor = inp_tensor\n",
    "    scores = torch.tensor(scores).view(-1,1)\n",
    "\n",
    "    # Split data into train, test and val batches\n",
    "    inp_tensor_batches = torch.split(inp_tensor, BATCH_N)\n",
    "    scores_batches = torch.split(scores, BATCH_N)\n",
    "#     X_train_val, X_test, y_train_val, y_test = train_test_split(inp_tensor_batches, scores_batches, shuffle=False, test_size=0.1)\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, shuffle = False, test_size=1/9)\n",
    "    \n",
    "    \n",
    "    #print(len(inp_tensor_batches), inp_tensor_batches[0].shape)\n",
    "\n",
    "    # Create attention masks\n",
    "    X_mask = attention_mask(inp_tensor_batches)\n",
    "    #print(len(X_mask), X_mask[0].shape)\n",
    "\n",
    "    # Batch X, mask and y together\n",
    "    batches = [(X, mask, y) for X,mask,y in zip(inp_tensor_batches, X_mask, scores_batches)]\n",
    "\n",
    "\n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "#bert_config = 'bert-base-multilingual-cased'\n",
    "BATCH_N = 20\n",
    "EPOCHS = 20\n",
    "LR = 2e-5\n",
    "\n",
    "# Set seeds\n",
    "seed_val = 111\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Preprocessing\n",
    "\n",
    "\n",
    "batches_en_train = get_batches(en_inputs, scores_train, BATCH_N)\n",
    "batches_de_train = get_batches(de_inputs, scores_train, BATCH_N)\n",
    "\n",
    "batches_en_val = get_batches(en_val_inputs, scores_val, BATCH_N)\n",
    "batches_de_val = get_batches(de_val_inputs, scores_val, BATCH_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batches_en_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(val_batches_en, val_batches_de):\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    eval_loss = 0\n",
    "    correlations = []\n",
    "\n",
    "    for i in range(len(val_batches_en)):     \n",
    "\n",
    "        batch_en = val_batches_en[i]\n",
    "        batch_de = val_batches_de[i]\n",
    "        \n",
    "        # Untie batch and put on GPU\n",
    "        X_en = batch_en[0].to(device=device, dtype=torch.long)\n",
    "        mask_en = batch_en[1].to(device)\n",
    "        \n",
    "        X_de = batch_de[0].to(device=device, dtype=torch.long)\n",
    "        mask_de = batch_de[1].to(device)\n",
    "        \n",
    "        y = batch_en[2].to(device)\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            y_pred = model(X_en, X_de)                      \n",
    "                \n",
    "        # Compute and record batch accuracy\n",
    "        corr = model.check_r(y_pred, y)\n",
    "        correlations.append(corr)\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(f\"|  Correlation: {np.mean(correlations):.2f}     |\")\n",
    "    print(f\"|  Validation took: {time.time() - t0:0f} s |\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def train(model, optimizer, train_batches_en, val_batches_en, train_batches_de, val_batches_de, epochs):\n",
    "\n",
    "    losses = []\n",
    "    \n",
    "    for epoch_i in range(epochs):\n",
    "        \n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        t0 = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "        for step in range(len(train_batches_en)):\n",
    "\n",
    "                       \n",
    "            batch_en = train_batches_en[step]\n",
    "            batch_de = train_batches_de[step]\n",
    "            \n",
    "            batch_en[0].requires_grad = True\n",
    "            batch_de[0].requires_grad = True\n",
    "\n",
    "            # Untie batch and put on GPU\n",
    "            X_en = batch_en[0].to(device=device, dtype=torch.long)\n",
    "            mask_en = batch_en[1].to(device)\n",
    "\n",
    "            X_de = batch_de[0].to(device=device, dtype=torch.long)\n",
    "            mask_de = batch_de[1].to(device)\n",
    "\n",
    "            y = batch_en[2].to(device)\n",
    "\n",
    "            model.zero_grad()                   # Reset grads\n",
    "            y_pred = model(X_en, X_de, mask_en, mask_de)             # Forward pass\n",
    "            loss = model.loss(y, y_pred)        # Compute loss\n",
    "            total_loss += loss.item()           # Accumulate loss\n",
    "            loss.backward()                     # Backward pass\n",
    "\n",
    "            optimizer.step()                    # Update params\n",
    "\n",
    "            torch.cuda.empty_cache()            # Clear GPU cache to avoid memory issues\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                # Progress update every 40 batches                \n",
    "                print('  Batch {:>5,}  of  {:>5,}    |    Elapsed: {:.0f}s.'.format(step, len(train_batches_en), time.time() - t0))\n",
    "                print(f'  Loss = {loss.item():.2f}')\n",
    "            \n",
    "        # Compute and store avg loss\n",
    "        avg_train_loss = total_loss / len(X_en)\n",
    "        losses.append(avg_train_loss)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:.0f}s\".format(time.time() - t0))\n",
    "\n",
    "        validation(val_batches_en, val_batches_de) \n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 20 ========\n",
      "Training...\n",
      "  Batch     0  of    350    |    Elapsed: 0s.\n",
      "  Loss = 1.55\n",
      "  Batch   100  of    350    |    Elapsed: 8s.\n",
      "  Loss = 0.66\n",
      "  Batch   200  of    350    |    Elapsed: 16s.\n",
      "  Loss = 0.50\n",
      "  Batch   300  of    350    |    Elapsed: 25s.\n",
      "  Loss = 0.42\n",
      "\n",
      "  Average training loss: 13.08\n",
      "  Training epoch took: 29s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: -0.03     |\n",
      "|  Validation took: 2.981506 s |\n",
      "\n",
      "======== Epoch 2 / 20 ========\n",
      "Training...\n",
      "  Batch     0  of    350    |    Elapsed: 0s.\n",
      "  Loss = 1.60\n",
      "  Batch   100  of    350    |    Elapsed: 8s.\n",
      "  Loss = 0.66\n",
      "  Batch   200  of    350    |    Elapsed: 16s.\n",
      "  Loss = 0.50\n",
      "  Batch   300  of    350    |    Elapsed: 25s.\n",
      "  Loss = 0.42\n",
      "\n",
      "  Average training loss: 13.06\n",
      "  Training epoch took: 29s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: -0.00     |\n",
      "|  Validation took: 3.127589 s |\n",
      "\n",
      "======== Epoch 3 / 20 ========\n",
      "Training...\n",
      "  Batch     0  of    350    |    Elapsed: 0s.\n",
      "  Loss = 1.60\n",
      "  Batch   100  of    350    |    Elapsed: 8s.\n",
      "  Loss = 0.66\n",
      "  Batch   200  of    350    |    Elapsed: 17s.\n",
      "  Loss = 0.50\n",
      "  Batch   300  of    350    |    Elapsed: 25s.\n",
      "  Loss = 0.42\n",
      "\n",
      "  Average training loss: 13.04\n",
      "  Training epoch took: 29s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.01     |\n",
      "|  Validation took: 2.996839 s |\n",
      "\n",
      "======== Epoch 4 / 20 ========\n",
      "Training...\n",
      "  Batch     0  of    350    |    Elapsed: 0s.\n",
      "  Loss = 1.60\n",
      "  Batch   100  of    350    |    Elapsed: 8s.\n",
      "  Loss = 0.66\n",
      "  Batch   200  of    350    |    Elapsed: 17s.\n",
      "  Loss = 0.50\n",
      "  Batch   300  of    350    |    Elapsed: 25s.\n",
      "  Loss = 0.42\n",
      "\n",
      "  Average training loss: 13.02\n",
      "  Training epoch took: 29s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.02     |\n",
      "|  Validation took: 3.006795 s |\n",
      "\n",
      "======== Epoch 5 / 20 ========\n",
      "Training...\n",
      "  Batch     0  of    350    |    Elapsed: 0s.\n",
      "  Loss = 1.59\n",
      "  Batch   100  of    350    |    Elapsed: 8s.\n",
      "  Loss = 0.66\n",
      "  Batch   200  of    350    |    Elapsed: 17s.\n",
      "  Loss = 0.50\n",
      "  Batch   300  of    350    |    Elapsed: 25s.\n",
      "  Loss = 0.42\n",
      "\n",
      "  Average training loss: 13.01\n",
      "  Training epoch took: 29s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.03     |\n",
      "|  Validation took: 2.985389 s |\n",
      "\n",
      "======== Epoch 6 / 20 ========\n",
      "Training...\n",
      "  Batch     0  of    350    |    Elapsed: 0s.\n",
      "  Loss = 1.59\n",
      "  Batch   100  of    350    |    Elapsed: 8s.\n",
      "  Loss = 0.66\n",
      "  Batch   200  of    350    |    Elapsed: 17s.\n",
      "  Loss = 0.50\n",
      "  Batch   300  of    350    |    Elapsed: 25s.\n",
      "  Loss = 0.42\n",
      "\n",
      "  Average training loss: 13.00\n",
      "  Training epoch took: 29s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.04     |\n",
      "|  Validation took: 3.026458 s |\n",
      "\n",
      "======== Epoch 7 / 20 ========\n",
      "Training...\n",
      "  Batch     0  of    350    |    Elapsed: 0s.\n",
      "  Loss = 1.59\n",
      "  Batch   100  of    350    |    Elapsed: 8s.\n",
      "  Loss = 0.66\n",
      "  Batch   200  of    350    |    Elapsed: 17s.\n",
      "  Loss = 0.50\n",
      "  Batch   300  of    350    |    Elapsed: 25s.\n",
      "  Loss = 0.42\n",
      "\n",
      "  Average training loss: 12.99\n",
      "  Training epoch took: 29s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.04     |\n",
      "|  Validation took: 2.987222 s |\n",
      "\n",
      "======== Epoch 8 / 20 ========\n",
      "Training...\n",
      "  Batch     0  of    350    |    Elapsed: 0s.\n",
      "  Loss = 1.59\n",
      "  Batch   100  of    350    |    Elapsed: 8s.\n",
      "  Loss = 0.66\n",
      "  Batch   200  of    350    |    Elapsed: 17s.\n",
      "  Loss = 0.50\n",
      "  Batch   300  of    350    |    Elapsed: 25s.\n",
      "  Loss = 0.42\n",
      "\n",
      "  Average training loss: 12.98\n",
      "  Training epoch took: 29s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.04     |\n",
      "|  Validation took: 2.987971 s |\n",
      "\n",
      "======== Epoch 9 / 20 ========\n",
      "Training...\n",
      "  Batch     0  of    350    |    Elapsed: 0s.\n",
      "  Loss = 1.58\n",
      "  Batch   100  of    350    |    Elapsed: 8s.\n",
      "  Loss = 0.66\n",
      "  Batch   200  of    350    |    Elapsed: 17s.\n",
      "  Loss = 0.50\n",
      "  Batch   300  of    350    |    Elapsed: 25s.\n",
      "  Loss = 0.42\n",
      "\n",
      "  Average training loss: 12.97\n",
      "  Training epoch took: 29s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.04     |\n",
      "|  Validation took: 2.989022 s |\n",
      "\n",
      "======== Epoch 10 / 20 ========\n",
      "Training...\n",
      "  Batch     0  of    350    |    Elapsed: 0s.\n",
      "  Loss = 1.58\n",
      "  Batch   100  of    350    |    Elapsed: 8s.\n",
      "  Loss = 0.66\n",
      "  Batch   200  of    350    |    Elapsed: 17s.\n",
      "  Loss = 0.50\n",
      "  Batch   300  of    350    |    Elapsed: 25s.\n",
      "  Loss = 0.42\n",
      "\n",
      "  Average training loss: 12.96\n",
      "  Training epoch took: 29s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.04     |\n",
      "|  Validation took: 2.993901 s |\n",
      "\n",
      "======== Epoch 11 / 20 ========\n",
      "Training...\n",
      "  Batch     0  of    350    |    Elapsed: 0s.\n",
      "  Loss = 1.58\n",
      "  Batch   100  of    350    |    Elapsed: 8s.\n",
      "  Loss = 0.66\n",
      "  Batch   200  of    350    |    Elapsed: 17s.\n",
      "  Loss = 0.50\n",
      "  Batch   300  of    350    |    Elapsed: 25s.\n",
      "  Loss = 0.41\n",
      "\n",
      "  Average training loss: 12.96\n",
      "  Training epoch took: 29s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.04     |\n",
      "|  Validation took: 2.986727 s |\n",
      "\n",
      "======== Epoch 12 / 20 ========\n",
      "Training...\n",
      "  Batch     0  of    350    |    Elapsed: 0s.\n",
      "  Loss = 1.58\n",
      "  Batch   100  of    350    |    Elapsed: 8s.\n",
      "  Loss = 0.66\n",
      "  Batch   200  of    350    |    Elapsed: 17s.\n",
      "  Loss = 0.50\n",
      "  Batch   300  of    350    |    Elapsed: 25s.\n",
      "  Loss = 0.41\n",
      "\n",
      "  Average training loss: 12.95\n",
      "  Training epoch took: 29s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.04     |\n",
      "|  Validation took: 2.991263 s |\n",
      "\n",
      "======== Epoch 13 / 20 ========\n",
      "Training...\n",
      "  Batch     0  of    350    |    Elapsed: 0s.\n",
      "  Loss = 1.58\n",
      "  Batch   100  of    350    |    Elapsed: 8s.\n",
      "  Loss = 0.66\n",
      "  Batch   200  of    350    |    Elapsed: 17s.\n",
      "  Loss = 0.50\n",
      "  Batch   300  of    350    |    Elapsed: 25s.\n",
      "  Loss = 0.41\n",
      "\n",
      "  Average training loss: 12.94\n",
      "  Training epoch took: 29s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.04     |\n",
      "|  Validation took: 2.989280 s |\n",
      "\n",
      "======== Epoch 14 / 20 ========\n",
      "Training...\n",
      "  Batch     0  of    350    |    Elapsed: 0s.\n",
      "  Loss = 1.57\n",
      "  Batch   100  of    350    |    Elapsed: 8s.\n",
      "  Loss = 0.66\n",
      "  Batch   200  of    350    |    Elapsed: 17s.\n",
      "  Loss = 0.50\n",
      "  Batch   300  of    350    |    Elapsed: 25s.\n",
      "  Loss = 0.41\n",
      "\n",
      "  Average training loss: 12.94\n",
      "  Training epoch took: 29s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.04     |\n",
      "|  Validation took: 2.993255 s |\n",
      "\n",
      "======== Epoch 15 / 20 ========\n",
      "Training...\n",
      "  Batch     0  of    350    |    Elapsed: 0s.\n",
      "  Loss = 1.57\n",
      "  Batch   100  of    350    |    Elapsed: 8s.\n",
      "  Loss = 0.66\n",
      "  Batch   200  of    350    |    Elapsed: 17s.\n",
      "  Loss = 0.50\n",
      "  Batch   300  of    350    |    Elapsed: 25s.\n",
      "  Loss = 0.41\n",
      "\n",
      "  Average training loss: 12.93\n",
      "  Training epoch took: 29s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.04     |\n",
      "|  Validation took: 2.988809 s |\n",
      "\n",
      "======== Epoch 16 / 20 ========\n",
      "Training...\n",
      "  Batch     0  of    350    |    Elapsed: 0s.\n",
      "  Loss = 1.57\n",
      "  Batch   100  of    350    |    Elapsed: 8s.\n",
      "  Loss = 0.66\n",
      "  Batch   200  of    350    |    Elapsed: 17s.\n",
      "  Loss = 0.50\n",
      "  Batch   300  of    350    |    Elapsed: 25s.\n",
      "  Loss = 0.41\n",
      "\n",
      "  Average training loss: 12.92\n",
      "  Training epoch took: 29s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.04     |\n",
      "|  Validation took: 2.987939 s |\n",
      "\n",
      "======== Epoch 17 / 20 ========\n",
      "Training...\n",
      "  Batch     0  of    350    |    Elapsed: 0s.\n",
      "  Loss = 1.57\n",
      "  Batch   100  of    350    |    Elapsed: 8s.\n",
      "  Loss = 0.66\n",
      "  Batch   200  of    350    |    Elapsed: 17s.\n",
      "  Loss = 0.50\n",
      "  Batch   300  of    350    |    Elapsed: 25s.\n",
      "  Loss = 0.41\n",
      "\n",
      "  Average training loss: 12.92\n",
      "  Training epoch took: 29s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.04     |\n",
      "|  Validation took: 2.990366 s |\n",
      "\n",
      "======== Epoch 18 / 20 ========\n",
      "Training...\n",
      "  Batch     0  of    350    |    Elapsed: 0s.\n",
      "  Loss = 1.57\n",
      "  Batch   100  of    350    |    Elapsed: 8s.\n",
      "  Loss = 0.66\n",
      "  Batch   200  of    350    |    Elapsed: 17s.\n",
      "  Loss = 0.50\n",
      "  Batch   300  of    350    |    Elapsed: 25s.\n",
      "  Loss = 0.41\n",
      "\n",
      "  Average training loss: 12.91\n",
      "  Training epoch took: 29s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.04     |\n",
      "|  Validation took: 2.987702 s |\n",
      "\n",
      "======== Epoch 19 / 20 ========\n",
      "Training...\n",
      "  Batch     0  of    350    |    Elapsed: 0s.\n",
      "  Loss = 1.57\n",
      "  Batch   100  of    350    |    Elapsed: 8s.\n",
      "  Loss = 0.66\n",
      "  Batch   200  of    350    |    Elapsed: 17s.\n",
      "  Loss = 0.50\n",
      "  Batch   300  of    350    |    Elapsed: 25s.\n",
      "  Loss = 0.41\n",
      "\n",
      "  Average training loss: 12.91\n",
      "  Training epoch took: 29s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.03     |\n",
      "|  Validation took: 2.989623 s |\n",
      "\n",
      "======== Epoch 20 / 20 ========\n",
      "Training...\n",
      "  Batch     0  of    350    |    Elapsed: 0s.\n",
      "  Loss = 1.56\n",
      "  Batch   100  of    350    |    Elapsed: 8s.\n",
      "  Loss = 0.66\n",
      "  Batch   200  of    350    |    Elapsed: 17s.\n",
      "  Loss = 0.50\n",
      "  Batch   300  of    350    |    Elapsed: 25s.\n",
      "  Loss = 0.41\n",
      "\n",
      "  Average training loss: 12.90\n",
      "  Training epoch took: 29s\n",
      "\n",
      "Running Validation...\n",
      "|  Correlation: 0.03     |\n",
      "|  Validation took: 2.988363 s |\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Model to train\n",
    "model = BertRegressor()\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=LR, eps=1e-8)\n",
    "total_steps = len(batches_en_train[0]) * EPOCHS // BATCH_N\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "#                                             num_warmup_steps = 0,\n",
    "#                                             num_training_steps = total_steps)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "train(model, optimizer, batches_en_train, batches_en_val, batches_de_train, batches_de_val, epochs=EPOCHS)\n",
    "torch.save(model.state_dict(), '..\\..\\Dump Files\\models\\model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37-msc",
   "language": "python",
   "name": "py37-msc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
