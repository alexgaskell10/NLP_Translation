{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt') # Download the tokenizer model\n",
    "# nltk.download('wordnet') # Download the wordnet corpora\n",
    "\n",
    "data_dir = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = open(data_dir + \"train.ende.src\", \"r\")\n",
    "train = f_train.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(strings):\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "    return [tokenizer.tokenize(l) for l in strings]\n",
    "\n",
    "\n",
    "def remove_stopwords(list_strings, additional_stopwords=(), language='english'):\n",
    "    words = stopwords.words('english')\n",
    "    for word in additional_stopwords:\n",
    "        if word not in words: words.append(word)\n",
    "\n",
    "    ret = []\n",
    "    for sentence in list_strings:\n",
    "        ret.append(list(filter(lambda word: word not in words, sentence)))\n",
    "    return ret\n",
    "\n",
    "def remove_capitalisation(list_strings):\n",
    "    ret = []\n",
    "    for sentence in list_strings:\n",
    "        ret.append([w.lower() for w in sentence])\n",
    "    return ret\n",
    "\n",
    "def remove_punctuation(list_strings):\n",
    "    ret = []\n",
    "    for sentence in list_strings:\n",
    "        ret.append(list(filter(lambda word: word not in string.punctuation, sentence)))\n",
    "    return ret\n",
    "\n",
    "\n",
    "def build_vocab(list_string):\n",
    "    vocab_set = set()\n",
    "    for sentence in list_string:\n",
    "        vocab_set.update(sentence)\n",
    "        \n",
    "    return vocab_set\n",
    "\n",
    "def build_w2i_i2w(vocabulary):\n",
    "    word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "    idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "    return word2idx, idx2word\n",
    "\n",
    "def lemmatise():\n",
    "    pass\n",
    "\n",
    "def stem():\n",
    "    pass\n",
    "\n",
    "def extract_context(list_string, word2idx, window_size=2):\n",
    "    idx_pairs = []\n",
    "    for sentence in list_string:\n",
    "        indices = [word2idx[word] for word in sentence]\n",
    "\n",
    "        for center_word_pos in range(len(indices)):\n",
    "\n",
    "            for w in range(-window_size, window_size + 1):\n",
    "                context_word_pos = center_word_pos + w\n",
    "\n",
    "                if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                    continue\n",
    "\n",
    "                context_word_idx = indices[context_word_pos]\n",
    "                idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "    return np.array(idx_pairs) # it will be useful to have this as numpy array\n",
    "\n",
    "\n",
    "def get_one_hot(vocabulary, word_idx):\n",
    "    x = torch.zeros(len(vocabulary)).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_tokenized = tokenize(train)\n",
    "train_nostop = remove_stopwords(train_tokenized)\n",
    "train_nopunct = remove_punctuation(train_nostop)\n",
    "train_nocap = remove_capitalisation(train_nopunct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc = build_vocab(train_nocap)\n",
    "word2idx, idx2word = build_w2i_i2w(voc)\n",
    "get_one_hot(voc, word2idx[\"how\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = extract_context(train_nocap, word2idx, window_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328830"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
